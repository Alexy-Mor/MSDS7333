{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP1sI4QoLEvc"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "from __future__ import print_function\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from tensorflow.python import keras\n",
        "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras import layers \n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import PrecisionRecallDisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbICnAk6PqUy",
        "outputId": "7fd19c5d-c41c-4a3f-f015-0aef388ffdd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Load up data and show.\n",
        " df = pd.read_csv('/content/drive/MyDrive/CS7-Final/final_project(5).csv')\n",
        " df.head()"
      ],
      "metadata": {
        "id": "GjFeHB9KbG2D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "de8895e9-43ae-49e1-98f3-4a47d93fe9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         x0        x1         x2        x3        x4         x5         x6  \\\n",
              "0 -0.166563 -3.961588   4.621113  2.481908 -1.800135   0.804684   6.718751   \n",
              "1 -0.149894 -0.585676  27.839856  4.152333  6.426802  -2.426943  40.477058   \n",
              "2 -0.321707 -1.429819  12.251561  6.586874 -5.304647 -11.311090  17.812850   \n",
              "3 -0.245594  5.076677 -24.149632  3.637307  6.505811   2.290224 -35.111751   \n",
              "4 -0.273366  0.306326 -11.352593  1.676758  2.928441  -0.616824 -16.505817   \n",
              "\n",
              "          x7        x8        x9  ...        x41       x42       x43  \\\n",
              "0 -14.789997 -1.040673 -4.204950  ...  -1.497117  5.414063 -2.325655   \n",
              "1  -6.725709  0.896421  0.330165  ...  36.292790  4.490915  0.762561   \n",
              "2  11.060572  5.325880 -2.632984  ...  -0.368491  9.088864 -0.689886   \n",
              "3 -18.913592 -0.337041 -5.568076  ...  15.691546 -7.467775  2.940789   \n",
              "4  27.532281  1.199715 -4.309105  ... -13.911297 -5.229937  1.783928   \n",
              "\n",
              "        x44       x45        x46       x47       x48        x49  y  \n",
              "0  1.674827 -0.264332  60.781427 -7.689696  0.151589  -8.040166  0  \n",
              "1  6.526662  1.007927  15.805696 -4.896678 -0.320283  16.719974  0  \n",
              "2 -2.731118  0.754200  30.856417 -7.428573 -2.090804  -7.869421  0  \n",
              "3 -6.424112  0.419776 -72.424569  5.361375  1.806070  -7.670847  0  \n",
              "4  3.957801 -0.096988 -14.085435 -0.208351 -0.894942  15.724742  1  \n",
              "\n",
              "[5 rows x 51 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a29021f-0605-4f69-a1d6-0487d1c2c446\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x0</th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>x3</th>\n",
              "      <th>x4</th>\n",
              "      <th>x5</th>\n",
              "      <th>x6</th>\n",
              "      <th>x7</th>\n",
              "      <th>x8</th>\n",
              "      <th>x9</th>\n",
              "      <th>...</th>\n",
              "      <th>x41</th>\n",
              "      <th>x42</th>\n",
              "      <th>x43</th>\n",
              "      <th>x44</th>\n",
              "      <th>x45</th>\n",
              "      <th>x46</th>\n",
              "      <th>x47</th>\n",
              "      <th>x48</th>\n",
              "      <th>x49</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.166563</td>\n",
              "      <td>-3.961588</td>\n",
              "      <td>4.621113</td>\n",
              "      <td>2.481908</td>\n",
              "      <td>-1.800135</td>\n",
              "      <td>0.804684</td>\n",
              "      <td>6.718751</td>\n",
              "      <td>-14.789997</td>\n",
              "      <td>-1.040673</td>\n",
              "      <td>-4.204950</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.497117</td>\n",
              "      <td>5.414063</td>\n",
              "      <td>-2.325655</td>\n",
              "      <td>1.674827</td>\n",
              "      <td>-0.264332</td>\n",
              "      <td>60.781427</td>\n",
              "      <td>-7.689696</td>\n",
              "      <td>0.151589</td>\n",
              "      <td>-8.040166</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.149894</td>\n",
              "      <td>-0.585676</td>\n",
              "      <td>27.839856</td>\n",
              "      <td>4.152333</td>\n",
              "      <td>6.426802</td>\n",
              "      <td>-2.426943</td>\n",
              "      <td>40.477058</td>\n",
              "      <td>-6.725709</td>\n",
              "      <td>0.896421</td>\n",
              "      <td>0.330165</td>\n",
              "      <td>...</td>\n",
              "      <td>36.292790</td>\n",
              "      <td>4.490915</td>\n",
              "      <td>0.762561</td>\n",
              "      <td>6.526662</td>\n",
              "      <td>1.007927</td>\n",
              "      <td>15.805696</td>\n",
              "      <td>-4.896678</td>\n",
              "      <td>-0.320283</td>\n",
              "      <td>16.719974</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.321707</td>\n",
              "      <td>-1.429819</td>\n",
              "      <td>12.251561</td>\n",
              "      <td>6.586874</td>\n",
              "      <td>-5.304647</td>\n",
              "      <td>-11.311090</td>\n",
              "      <td>17.812850</td>\n",
              "      <td>11.060572</td>\n",
              "      <td>5.325880</td>\n",
              "      <td>-2.632984</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.368491</td>\n",
              "      <td>9.088864</td>\n",
              "      <td>-0.689886</td>\n",
              "      <td>-2.731118</td>\n",
              "      <td>0.754200</td>\n",
              "      <td>30.856417</td>\n",
              "      <td>-7.428573</td>\n",
              "      <td>-2.090804</td>\n",
              "      <td>-7.869421</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.245594</td>\n",
              "      <td>5.076677</td>\n",
              "      <td>-24.149632</td>\n",
              "      <td>3.637307</td>\n",
              "      <td>6.505811</td>\n",
              "      <td>2.290224</td>\n",
              "      <td>-35.111751</td>\n",
              "      <td>-18.913592</td>\n",
              "      <td>-0.337041</td>\n",
              "      <td>-5.568076</td>\n",
              "      <td>...</td>\n",
              "      <td>15.691546</td>\n",
              "      <td>-7.467775</td>\n",
              "      <td>2.940789</td>\n",
              "      <td>-6.424112</td>\n",
              "      <td>0.419776</td>\n",
              "      <td>-72.424569</td>\n",
              "      <td>5.361375</td>\n",
              "      <td>1.806070</td>\n",
              "      <td>-7.670847</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.273366</td>\n",
              "      <td>0.306326</td>\n",
              "      <td>-11.352593</td>\n",
              "      <td>1.676758</td>\n",
              "      <td>2.928441</td>\n",
              "      <td>-0.616824</td>\n",
              "      <td>-16.505817</td>\n",
              "      <td>27.532281</td>\n",
              "      <td>1.199715</td>\n",
              "      <td>-4.309105</td>\n",
              "      <td>...</td>\n",
              "      <td>-13.911297</td>\n",
              "      <td>-5.229937</td>\n",
              "      <td>1.783928</td>\n",
              "      <td>3.957801</td>\n",
              "      <td>-0.096988</td>\n",
              "      <td>-14.085435</td>\n",
              "      <td>-0.208351</td>\n",
              "      <td>-0.894942</td>\n",
              "      <td>15.724742</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 51 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a29021f-0605-4f69-a1d6-0487d1c2c446')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2a29021f-0605-4f69-a1d6-0487d1c2c446 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2a29021f-0605-4f69-a1d6-0487d1c2c446');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JEtQVRz88FVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest and Neural network"
      ],
      "metadata": {
        "id": "IyRLRXv96__Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "HeVgkR9EPulN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd37ac2-c162-4b67-885e-b4c5449061a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 160000 entries, 0 to 159999\n",
            "Data columns (total 51 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   x0      159974 non-null  float64\n",
            " 1   x1      159975 non-null  float64\n",
            " 2   x2      159962 non-null  float64\n",
            " 3   x3      159963 non-null  float64\n",
            " 4   x4      159974 non-null  float64\n",
            " 5   x5      159963 non-null  float64\n",
            " 6   x6      159974 non-null  float64\n",
            " 7   x7      159973 non-null  float64\n",
            " 8   x8      159979 non-null  float64\n",
            " 9   x9      159970 non-null  float64\n",
            " 10  x10     159957 non-null  float64\n",
            " 11  x11     159970 non-null  float64\n",
            " 12  x12     159964 non-null  float64\n",
            " 13  x13     159969 non-null  float64\n",
            " 14  x14     159966 non-null  float64\n",
            " 15  x15     159965 non-null  float64\n",
            " 16  x16     159974 non-null  float64\n",
            " 17  x17     159973 non-null  float64\n",
            " 18  x18     159960 non-null  float64\n",
            " 19  x19     159965 non-null  float64\n",
            " 20  x20     159962 non-null  float64\n",
            " 21  x21     159971 non-null  float64\n",
            " 22  x22     159973 non-null  float64\n",
            " 23  x23     159953 non-null  float64\n",
            " 24  x24     159972 non-null  object \n",
            " 25  x25     159978 non-null  float64\n",
            " 26  x26     159964 non-null  float64\n",
            " 27  x27     159970 non-null  float64\n",
            " 28  x28     159965 non-null  float64\n",
            " 29  x29     159970 non-null  object \n",
            " 30  x30     159970 non-null  object \n",
            " 31  x31     159961 non-null  float64\n",
            " 32  x32     159969 non-null  object \n",
            " 33  x33     159959 non-null  float64\n",
            " 34  x34     159959 non-null  float64\n",
            " 35  x35     159970 non-null  float64\n",
            " 36  x36     159973 non-null  float64\n",
            " 37  x37     159977 non-null  object \n",
            " 38  x38     159969 non-null  float64\n",
            " 39  x39     159977 non-null  float64\n",
            " 40  x40     159964 non-null  float64\n",
            " 41  x41     159960 non-null  float64\n",
            " 42  x42     159974 non-null  float64\n",
            " 43  x43     159963 non-null  float64\n",
            " 44  x44     159960 non-null  float64\n",
            " 45  x45     159971 non-null  float64\n",
            " 46  x46     159969 non-null  float64\n",
            " 47  x47     159963 non-null  float64\n",
            " 48  x48     159968 non-null  float64\n",
            " 49  x49     159968 non-null  float64\n",
            " 50  y       160000 non-null  int64  \n",
            "dtypes: float64(45), int64(1), object(5)\n",
            "memory usage: 62.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x24'].value_counts() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa57ec4-3171-4bb6-f04a-90665d313e5a",
        "id": "30J81TD6YteD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "asia       138965\n",
              "euorpe      16538\n",
              "america      4469\n",
              "Name: x24, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x29'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a996554-f66f-4a65-e1e1-fabcbd9a136d",
        "id": "HFmgsVH4YhH_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "July       45569\n",
              "Jun        41329\n",
              "Aug        29406\n",
              "May        21939\n",
              "sept.      10819\n",
              "Apr         6761\n",
              "Oct         2407\n",
              "Mar         1231\n",
              "Nov          337\n",
              "Feb          140\n",
              "Dev           23\n",
              "January        9\n",
              "Name: x29, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x29'] = df['x29'].replace(['July'], 'Jul')\n",
        "df['x29'] = df['x29'].replace(['sept.'], 'Sep')\n",
        "df['x29'] = df['x29'].replace(['Dev'], 'Dec')\n",
        "df['x29'] = df['x29'].replace(['January'], 'Jan')"
      ],
      "metadata": {
        "id": "pFrjVAlMY7qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['x30'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c65cdc-558d-4c04-d1a0-9e43091de76b",
        "id": "cSiULvHlX68z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "wednesday    101535\n",
              "thurday       29429\n",
              "tuesday       27954\n",
              "friday          564\n",
              "monday          488\n",
              "Name: x30, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x30'] = df['x30'].replace(['wednesday'], 'wed')\n",
        "df['x30'] = df['x30'].replace(['thurday'], 'thu')\n",
        "df['x30'] = df['x30'].replace(['tuesday'], 'tue')\n",
        "df['x30'] = df['x30'].replace(['friday'], 'fri')\n",
        "df['x30'] = df['x30'].replace(['monday'], 'mon')"
      ],
      "metadata": {
        "id": "NsWPz52SahpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['x32'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e4ff20-381b-49b8-954b-8cd36a84b535",
        "id": "ypQJTtIUYYZK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01%     40767\n",
              "-0.01%    34094\n",
              "0.0%      33923\n",
              "-0.0%     30492\n",
              "-0.02%     9924\n",
              "0.02%      7987\n",
              "-0.03%     1727\n",
              "0.03%       855\n",
              "-0.04%      138\n",
              "0.04%        55\n",
              "-0.05%        6\n",
              "0.05%         1\n",
              "Name: x32, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x32'] = df['x32'].astype(str)"
      ],
      "metadata": {
        "id": "SNs4g_b37l3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['x32'] = df['x32'].str.replace('%', '')\n",
        "\n",
        "df['x32'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_vZtv4j7jn-",
        "outputId": "da5f6db4-3a1f-4739-9f6c-c814b3b3fe63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0.0\n",
              "1    -0.02\n",
              "2    -0.01\n",
              "3     0.01\n",
              "4     0.01\n",
              "Name: x32, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x32'] = df['x32'].astype('float64')"
      ],
      "metadata": {
        "id": "id95zg-97ueC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['x37'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2Sn728NXNXm",
        "outputId": "bdfb289a-d46e-4241-c26f-86c205beb3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "$-311.26     6\n",
              "$-336.77     6\n",
              "$237.4       6\n",
              "$72.42       6\n",
              "$341.26      6\n",
              "            ..\n",
              "$-505.21     1\n",
              "$770.07      1\n",
              "$74.62       1\n",
              "$-1082.96    1\n",
              "$-1229.34    1\n",
              "Name: x37, Length: 129198, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x37'] = df['x37'].astype(str)"
      ],
      "metadata": {
        "id": "6FtgIr5X-fFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['x37'] = df['x37'].str.replace('$', '')\n",
        "\n",
        "df['x37'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDxnhCGOBdDd",
        "outputId": "76729f20-7cdc-46d7-b5f4-80155822624f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-7a1d49381926>:1: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df['x37'] = df['x37'].str.replace('$', '')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     1313.96\n",
              "1     1962.78\n",
              "2      430.47\n",
              "3    -2366.29\n",
              "4     -620.66\n",
              "Name: x37, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x37'] = df['x37'].astype('float64')"
      ],
      "metadata": {
        "id": "OnhJQd2OCV0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "zHZOvJKG9MbW",
        "outputId": "4e43c84f-8563-4928-ce63-b9c3bebf8775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  x0             x1             x2             x3  \\\n",
              "count  159974.000000  159975.000000  159962.000000  159963.000000   \n",
              "mean       -0.001028       0.001358      -1.150145      -0.024637   \n",
              "std         0.371137       6.340632      13.273480       8.065032   \n",
              "min        -1.592635     -26.278302     -59.394048     -35.476594   \n",
              "25%        -0.251641      -4.260973     -10.166536      -5.454438   \n",
              "50%        -0.002047       0.004813      -1.340932      -0.031408   \n",
              "75%         0.248532       4.284220       7.871676       5.445179   \n",
              "max         1.600849      27.988178      63.545653      38.906025   \n",
              "\n",
              "                  x4             x5             x6             x7  \\\n",
              "count  159974.000000  159963.000000  159974.000000  159973.000000   \n",
              "mean       -0.000549       0.013582      -1.670670      -7.692795   \n",
              "std         6.382293       7.670076      19.298665      30.542264   \n",
              "min       -28.467536     -33.822988     -86.354483    -181.506976   \n",
              "25%        -4.313118      -5.148130     -14.780146     -27.324771   \n",
              "50%         0.000857       0.014118      -1.948594      -6.956789   \n",
              "75%         4.306660       5.190749      11.446931      12.217071   \n",
              "max        26.247812      35.550110      92.390605     149.150634   \n",
              "\n",
              "                  x8             x9  ...            x41            x42  \\\n",
              "count  159979.000000  159970.000000  ...  159960.000000  159974.000000   \n",
              "mean       -0.030540       0.005462  ...       6.701076      -1.833820   \n",
              "std         8.901185       6.355040  ...      18.680196       5.110705   \n",
              "min       -37.691045     -27.980659  ...     -82.167224     -27.933750   \n",
              "25%        -6.031058      -4.260619  ...      -5.804080      -5.162869   \n",
              "50%        -0.016840       0.006045  ...       6.840110      -1.923754   \n",
              "75%         5.972349       4.305734  ...      19.266367       1.453507   \n",
              "max        39.049831      27.377842  ...     100.050432      22.668041   \n",
              "\n",
              "                 x43            x44            x45            x46  \\\n",
              "count  159963.000000  159960.000000  159971.000000  159969.000000   \n",
              "mean       -0.002091      -0.006250       0.000885     -12.755395   \n",
              "std         1.534952       4.164595       0.396621      36.608641   \n",
              "min        -6.876234     -17.983487      -1.753221    -201.826828   \n",
              "25%        -1.039677      -2.812055      -0.266518     -36.428329   \n",
              "50%        -0.004385      -0.010484       0.001645     -12.982497   \n",
              "75%         1.033275       2.783274       0.269049      11.445443   \n",
              "max         6.680922      19.069759       1.669205     150.859415   \n",
              "\n",
              "                 x47            x48            x49              y  \n",
              "count  159963.000000  159968.000000  159968.000000  160000.000000  \n",
              "mean        0.028622      -0.000224      -0.674224       0.401231  \n",
              "std         4.788157       1.935501      15.036738       0.490149  \n",
              "min       -21.086333      -8.490155     -65.791191       0.000000  \n",
              "25%        -3.216016      -1.320800     -10.931753       0.000000  \n",
              "50%         0.035865      -0.011993      -0.574410       0.000000  \n",
              "75%         3.268028       1.317703       9.651072       1.000000  \n",
              "max        20.836854       8.226552      66.877604       1.000000  \n",
              "\n",
              "[8 rows x 48 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4ff7c8bc-11b2-4487-8b2c-bc7bc9cc6ebb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x0</th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>x3</th>\n",
              "      <th>x4</th>\n",
              "      <th>x5</th>\n",
              "      <th>x6</th>\n",
              "      <th>x7</th>\n",
              "      <th>x8</th>\n",
              "      <th>x9</th>\n",
              "      <th>...</th>\n",
              "      <th>x41</th>\n",
              "      <th>x42</th>\n",
              "      <th>x43</th>\n",
              "      <th>x44</th>\n",
              "      <th>x45</th>\n",
              "      <th>x46</th>\n",
              "      <th>x47</th>\n",
              "      <th>x48</th>\n",
              "      <th>x49</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>159974.000000</td>\n",
              "      <td>159975.000000</td>\n",
              "      <td>159962.000000</td>\n",
              "      <td>159963.000000</td>\n",
              "      <td>159974.000000</td>\n",
              "      <td>159963.000000</td>\n",
              "      <td>159974.000000</td>\n",
              "      <td>159973.000000</td>\n",
              "      <td>159979.000000</td>\n",
              "      <td>159970.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>159960.000000</td>\n",
              "      <td>159974.000000</td>\n",
              "      <td>159963.000000</td>\n",
              "      <td>159960.000000</td>\n",
              "      <td>159971.000000</td>\n",
              "      <td>159969.000000</td>\n",
              "      <td>159963.000000</td>\n",
              "      <td>159968.000000</td>\n",
              "      <td>159968.000000</td>\n",
              "      <td>160000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-0.001028</td>\n",
              "      <td>0.001358</td>\n",
              "      <td>-1.150145</td>\n",
              "      <td>-0.024637</td>\n",
              "      <td>-0.000549</td>\n",
              "      <td>0.013582</td>\n",
              "      <td>-1.670670</td>\n",
              "      <td>-7.692795</td>\n",
              "      <td>-0.030540</td>\n",
              "      <td>0.005462</td>\n",
              "      <td>...</td>\n",
              "      <td>6.701076</td>\n",
              "      <td>-1.833820</td>\n",
              "      <td>-0.002091</td>\n",
              "      <td>-0.006250</td>\n",
              "      <td>0.000885</td>\n",
              "      <td>-12.755395</td>\n",
              "      <td>0.028622</td>\n",
              "      <td>-0.000224</td>\n",
              "      <td>-0.674224</td>\n",
              "      <td>0.401231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.371137</td>\n",
              "      <td>6.340632</td>\n",
              "      <td>13.273480</td>\n",
              "      <td>8.065032</td>\n",
              "      <td>6.382293</td>\n",
              "      <td>7.670076</td>\n",
              "      <td>19.298665</td>\n",
              "      <td>30.542264</td>\n",
              "      <td>8.901185</td>\n",
              "      <td>6.355040</td>\n",
              "      <td>...</td>\n",
              "      <td>18.680196</td>\n",
              "      <td>5.110705</td>\n",
              "      <td>1.534952</td>\n",
              "      <td>4.164595</td>\n",
              "      <td>0.396621</td>\n",
              "      <td>36.608641</td>\n",
              "      <td>4.788157</td>\n",
              "      <td>1.935501</td>\n",
              "      <td>15.036738</td>\n",
              "      <td>0.490149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-1.592635</td>\n",
              "      <td>-26.278302</td>\n",
              "      <td>-59.394048</td>\n",
              "      <td>-35.476594</td>\n",
              "      <td>-28.467536</td>\n",
              "      <td>-33.822988</td>\n",
              "      <td>-86.354483</td>\n",
              "      <td>-181.506976</td>\n",
              "      <td>-37.691045</td>\n",
              "      <td>-27.980659</td>\n",
              "      <td>...</td>\n",
              "      <td>-82.167224</td>\n",
              "      <td>-27.933750</td>\n",
              "      <td>-6.876234</td>\n",
              "      <td>-17.983487</td>\n",
              "      <td>-1.753221</td>\n",
              "      <td>-201.826828</td>\n",
              "      <td>-21.086333</td>\n",
              "      <td>-8.490155</td>\n",
              "      <td>-65.791191</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.251641</td>\n",
              "      <td>-4.260973</td>\n",
              "      <td>-10.166536</td>\n",
              "      <td>-5.454438</td>\n",
              "      <td>-4.313118</td>\n",
              "      <td>-5.148130</td>\n",
              "      <td>-14.780146</td>\n",
              "      <td>-27.324771</td>\n",
              "      <td>-6.031058</td>\n",
              "      <td>-4.260619</td>\n",
              "      <td>...</td>\n",
              "      <td>-5.804080</td>\n",
              "      <td>-5.162869</td>\n",
              "      <td>-1.039677</td>\n",
              "      <td>-2.812055</td>\n",
              "      <td>-0.266518</td>\n",
              "      <td>-36.428329</td>\n",
              "      <td>-3.216016</td>\n",
              "      <td>-1.320800</td>\n",
              "      <td>-10.931753</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-0.002047</td>\n",
              "      <td>0.004813</td>\n",
              "      <td>-1.340932</td>\n",
              "      <td>-0.031408</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>0.014118</td>\n",
              "      <td>-1.948594</td>\n",
              "      <td>-6.956789</td>\n",
              "      <td>-0.016840</td>\n",
              "      <td>0.006045</td>\n",
              "      <td>...</td>\n",
              "      <td>6.840110</td>\n",
              "      <td>-1.923754</td>\n",
              "      <td>-0.004385</td>\n",
              "      <td>-0.010484</td>\n",
              "      <td>0.001645</td>\n",
              "      <td>-12.982497</td>\n",
              "      <td>0.035865</td>\n",
              "      <td>-0.011993</td>\n",
              "      <td>-0.574410</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.248532</td>\n",
              "      <td>4.284220</td>\n",
              "      <td>7.871676</td>\n",
              "      <td>5.445179</td>\n",
              "      <td>4.306660</td>\n",
              "      <td>5.190749</td>\n",
              "      <td>11.446931</td>\n",
              "      <td>12.217071</td>\n",
              "      <td>5.972349</td>\n",
              "      <td>4.305734</td>\n",
              "      <td>...</td>\n",
              "      <td>19.266367</td>\n",
              "      <td>1.453507</td>\n",
              "      <td>1.033275</td>\n",
              "      <td>2.783274</td>\n",
              "      <td>0.269049</td>\n",
              "      <td>11.445443</td>\n",
              "      <td>3.268028</td>\n",
              "      <td>1.317703</td>\n",
              "      <td>9.651072</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.600849</td>\n",
              "      <td>27.988178</td>\n",
              "      <td>63.545653</td>\n",
              "      <td>38.906025</td>\n",
              "      <td>26.247812</td>\n",
              "      <td>35.550110</td>\n",
              "      <td>92.390605</td>\n",
              "      <td>149.150634</td>\n",
              "      <td>39.049831</td>\n",
              "      <td>27.377842</td>\n",
              "      <td>...</td>\n",
              "      <td>100.050432</td>\n",
              "      <td>22.668041</td>\n",
              "      <td>6.680922</td>\n",
              "      <td>19.069759</td>\n",
              "      <td>1.669205</td>\n",
              "      <td>150.859415</td>\n",
              "      <td>20.836854</td>\n",
              "      <td>8.226552</td>\n",
              "      <td>66.877604</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 48 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ff7c8bc-11b2-4487-8b2c-bc7bc9cc6ebb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4ff7c8bc-11b2-4487-8b2c-bc7bc9cc6ebb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4ff7c8bc-11b2-4487-8b2c-bc7bc9cc6ebb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUQoWh-V8Qdg",
        "outputId": "21ae4d02-01fc-4aa9-e2b0-92fd749052ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 160000 entries, 0 to 159999\n",
            "Data columns (total 51 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   x0      159974 non-null  float64\n",
            " 1   x1      159975 non-null  float64\n",
            " 2   x2      159962 non-null  float64\n",
            " 3   x3      159963 non-null  float64\n",
            " 4   x4      159974 non-null  float64\n",
            " 5   x5      159963 non-null  float64\n",
            " 6   x6      159974 non-null  float64\n",
            " 7   x7      159973 non-null  float64\n",
            " 8   x8      159979 non-null  float64\n",
            " 9   x9      159970 non-null  float64\n",
            " 10  x10     159957 non-null  float64\n",
            " 11  x11     159970 non-null  float64\n",
            " 12  x12     159964 non-null  float64\n",
            " 13  x13     159969 non-null  float64\n",
            " 14  x14     159966 non-null  float64\n",
            " 15  x15     159965 non-null  float64\n",
            " 16  x16     159974 non-null  float64\n",
            " 17  x17     159973 non-null  float64\n",
            " 18  x18     159960 non-null  float64\n",
            " 19  x19     159965 non-null  float64\n",
            " 20  x20     159962 non-null  float64\n",
            " 21  x21     159971 non-null  float64\n",
            " 22  x22     159973 non-null  float64\n",
            " 23  x23     159953 non-null  float64\n",
            " 24  x24     159972 non-null  object \n",
            " 25  x25     159978 non-null  float64\n",
            " 26  x26     159964 non-null  float64\n",
            " 27  x27     159970 non-null  float64\n",
            " 28  x28     159965 non-null  float64\n",
            " 29  x29     159970 non-null  object \n",
            " 30  x30     159970 non-null  object \n",
            " 31  x31     159961 non-null  float64\n",
            " 32  x32     159969 non-null  float64\n",
            " 33  x33     159959 non-null  float64\n",
            " 34  x34     159959 non-null  float64\n",
            " 35  x35     159970 non-null  float64\n",
            " 36  x36     159973 non-null  float64\n",
            " 37  x37     159977 non-null  float64\n",
            " 38  x38     159969 non-null  float64\n",
            " 39  x39     159977 non-null  float64\n",
            " 40  x40     159964 non-null  float64\n",
            " 41  x41     159960 non-null  float64\n",
            " 42  x42     159974 non-null  float64\n",
            " 43  x43     159963 non-null  float64\n",
            " 44  x44     159960 non-null  float64\n",
            " 45  x45     159971 non-null  float64\n",
            " 46  x46     159969 non-null  float64\n",
            " 47  x47     159963 non-null  float64\n",
            " 48  x48     159968 non-null  float64\n",
            " 49  x49     159968 non-null  float64\n",
            " 50  y       160000 non-null  int64  \n",
            "dtypes: float64(47), int64(1), object(3)\n",
            "memory usage: 62.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percent_missing = df.isnull().sum() * 100 / len(df)\n",
        "missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
        "                                 'percent_missing': percent_missing})"
      ],
      "metadata": {
        "id": "-isA2bxo9blK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_value_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qgXs75fX_FLF",
        "outputId": "fdd2798a-c9ba-4c3f-a80c-75ac8d6eba4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    column_name  percent_missing\n",
              "y             y         0.000000\n",
              "x8           x8         0.013125\n",
              "x25         x25         0.013750\n",
              "x37         x37         0.014375\n",
              "x39         x39         0.014375\n",
              "x1           x1         0.015625\n",
              "x0           x0         0.016250\n",
              "x16         x16         0.016250\n",
              "x42         x42         0.016250\n",
              "x4           x4         0.016250\n",
              "x6           x6         0.016250\n",
              "x7           x7         0.016875\n",
              "x22         x22         0.016875\n",
              "x36         x36         0.016875\n",
              "x17         x17         0.016875\n",
              "x24         x24         0.017500\n",
              "x21         x21         0.018125\n",
              "x45         x45         0.018125\n",
              "x30         x30         0.018750\n",
              "x29         x29         0.018750\n",
              "x27         x27         0.018750\n",
              "x9           x9         0.018750\n",
              "x11         x11         0.018750\n",
              "x35         x35         0.018750\n",
              "x32         x32         0.019375\n",
              "x46         x46         0.019375\n",
              "x13         x13         0.019375\n",
              "x38         x38         0.019375\n",
              "x48         x48         0.020000\n",
              "x49         x49         0.020000\n",
              "x14         x14         0.021250\n",
              "x19         x19         0.021875\n",
              "x15         x15         0.021875\n",
              "x28         x28         0.021875\n",
              "x40         x40         0.022500\n",
              "x26         x26         0.022500\n",
              "x12         x12         0.022500\n",
              "x3           x3         0.023125\n",
              "x5           x5         0.023125\n",
              "x47         x47         0.023125\n",
              "x43         x43         0.023125\n",
              "x2           x2         0.023750\n",
              "x20         x20         0.023750\n",
              "x31         x31         0.024375\n",
              "x41         x41         0.025000\n",
              "x18         x18         0.025000\n",
              "x44         x44         0.025000\n",
              "x33         x33         0.025625\n",
              "x34         x34         0.025625\n",
              "x10         x10         0.026875\n",
              "x23         x23         0.029375"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5dfaffc4-e0a3-4180-adf4-c692c8c8984c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>column_name</th>\n",
              "      <th>percent_missing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>y</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x8</th>\n",
              "      <td>x8</td>\n",
              "      <td>0.013125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x25</th>\n",
              "      <td>x25</td>\n",
              "      <td>0.013750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x37</th>\n",
              "      <td>x37</td>\n",
              "      <td>0.014375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x39</th>\n",
              "      <td>x39</td>\n",
              "      <td>0.014375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x1</th>\n",
              "      <td>x1</td>\n",
              "      <td>0.015625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x0</th>\n",
              "      <td>x0</td>\n",
              "      <td>0.016250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x16</th>\n",
              "      <td>x16</td>\n",
              "      <td>0.016250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x42</th>\n",
              "      <td>x42</td>\n",
              "      <td>0.016250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x4</th>\n",
              "      <td>x4</td>\n",
              "      <td>0.016250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x6</th>\n",
              "      <td>x6</td>\n",
              "      <td>0.016250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x7</th>\n",
              "      <td>x7</td>\n",
              "      <td>0.016875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x22</th>\n",
              "      <td>x22</td>\n",
              "      <td>0.016875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x36</th>\n",
              "      <td>x36</td>\n",
              "      <td>0.016875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x17</th>\n",
              "      <td>x17</td>\n",
              "      <td>0.016875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x24</th>\n",
              "      <td>x24</td>\n",
              "      <td>0.017500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x21</th>\n",
              "      <td>x21</td>\n",
              "      <td>0.018125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x45</th>\n",
              "      <td>x45</td>\n",
              "      <td>0.018125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x30</th>\n",
              "      <td>x30</td>\n",
              "      <td>0.018750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x29</th>\n",
              "      <td>x29</td>\n",
              "      <td>0.018750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x27</th>\n",
              "      <td>x27</td>\n",
              "      <td>0.018750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x9</th>\n",
              "      <td>x9</td>\n",
              "      <td>0.018750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x11</th>\n",
              "      <td>x11</td>\n",
              "      <td>0.018750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x35</th>\n",
              "      <td>x35</td>\n",
              "      <td>0.018750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x32</th>\n",
              "      <td>x32</td>\n",
              "      <td>0.019375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x46</th>\n",
              "      <td>x46</td>\n",
              "      <td>0.019375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x13</th>\n",
              "      <td>x13</td>\n",
              "      <td>0.019375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x38</th>\n",
              "      <td>x38</td>\n",
              "      <td>0.019375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x48</th>\n",
              "      <td>x48</td>\n",
              "      <td>0.020000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x49</th>\n",
              "      <td>x49</td>\n",
              "      <td>0.020000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x14</th>\n",
              "      <td>x14</td>\n",
              "      <td>0.021250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x19</th>\n",
              "      <td>x19</td>\n",
              "      <td>0.021875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x15</th>\n",
              "      <td>x15</td>\n",
              "      <td>0.021875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x28</th>\n",
              "      <td>x28</td>\n",
              "      <td>0.021875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x40</th>\n",
              "      <td>x40</td>\n",
              "      <td>0.022500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x26</th>\n",
              "      <td>x26</td>\n",
              "      <td>0.022500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x12</th>\n",
              "      <td>x12</td>\n",
              "      <td>0.022500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x3</th>\n",
              "      <td>x3</td>\n",
              "      <td>0.023125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x5</th>\n",
              "      <td>x5</td>\n",
              "      <td>0.023125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x47</th>\n",
              "      <td>x47</td>\n",
              "      <td>0.023125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x43</th>\n",
              "      <td>x43</td>\n",
              "      <td>0.023125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x2</th>\n",
              "      <td>x2</td>\n",
              "      <td>0.023750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x20</th>\n",
              "      <td>x20</td>\n",
              "      <td>0.023750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x31</th>\n",
              "      <td>x31</td>\n",
              "      <td>0.024375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x41</th>\n",
              "      <td>x41</td>\n",
              "      <td>0.025000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x18</th>\n",
              "      <td>x18</td>\n",
              "      <td>0.025000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x44</th>\n",
              "      <td>x44</td>\n",
              "      <td>0.025000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x33</th>\n",
              "      <td>x33</td>\n",
              "      <td>0.025625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x34</th>\n",
              "      <td>x34</td>\n",
              "      <td>0.025625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x10</th>\n",
              "      <td>x10</td>\n",
              "      <td>0.026875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x23</th>\n",
              "      <td>x23</td>\n",
              "      <td>0.029375</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5dfaffc4-e0a3-4180-adf4-c692c8c8984c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5dfaffc4-e0a3-4180-adf4-c692c8c8984c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5dfaffc4-e0a3-4180-adf4-c692c8c8984c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do0nOQwudRJg",
        "outputId": "bd0b6605-e106-4c40-cbdf-6e510e808d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160000, 51)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#No column has more than 3% can either drop or use mean/median?\n",
        "#will go with median fill  \n",
        "#Maybe leave it?\n",
        "\n",
        "def func(df):\n",
        "    df = df.copy()\n",
        "    for col in df:\n",
        "        # select only integer or float dtypes\n",
        "        if df[col].dtype in (\"int\", \"float\"):\n",
        "            df[col] = df[col].fillna(df[col].mode()[0])\n",
        "    return df\n",
        "\n",
        "df = func(df)"
      ],
      "metadata": {
        "id": "fJemBYr3Azfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['x24'] = df['x24'].fillna('unknown')\n",
        "df['x29'] = df['x29'].fillna('unknown')\n",
        "df['x30'] = df['x30'].fillna('unknown')"
      ],
      "metadata": {
        "id": "n6Ltk9rw9ilS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category = df.select_dtypes(exclude='number')\n",
        "category.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "YNsc-VJze9yl",
        "outputId": "933bcf9c-1398-4ba6-9e45-cb33e0862ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      x24  x29  x30\n",
              "0  euorpe  Jul  tue\n",
              "1    asia  Aug  wed\n",
              "2    asia  Jul  wed\n",
              "3    asia  Jul  wed\n",
              "4    asia  Jul  tue"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-83c7fe1c-11ca-4d23-a3fa-f23e279ce81b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x24</th>\n",
              "      <th>x29</th>\n",
              "      <th>x30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>euorpe</td>\n",
              "      <td>Jul</td>\n",
              "      <td>tue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>asia</td>\n",
              "      <td>Aug</td>\n",
              "      <td>wed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>asia</td>\n",
              "      <td>Jul</td>\n",
              "      <td>wed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>asia</td>\n",
              "      <td>Jul</td>\n",
              "      <td>wed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>asia</td>\n",
              "      <td>Jul</td>\n",
              "      <td>tue</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83c7fe1c-11ca-4d23-a3fa-f23e279ce81b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-83c7fe1c-11ca-4d23-a3fa-f23e279ce81b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-83c7fe1c-11ca-4d23-a3fa-f23e279ce81b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cat = OneHotEncoder().fit_transform(category)\n",
        "cat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roTQ4rZOgG6R",
        "outputId": "853b0a68-d99c-4978-94a0-a387feed5ca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<160000x23 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 480000 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat2 = pd.DataFrame.sparse.from_spmatrix(cat)"
      ],
      "metadata": {
        "id": "rfeI8ie4kvLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric = df.select_dtypes(include='number')\n",
        "numeric.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "i1Y58ectgOtF",
        "outputId": "c75a41af-4d29-4902-9144-1f5d77e7aef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         x0        x1         x2        x3        x4         x5         x6  \\\n",
              "0 -0.166563 -3.961588   4.621113  2.481908 -1.800135   0.804684   6.718751   \n",
              "1 -0.149894 -0.585676  27.839856  4.152333  6.426802  -2.426943  40.477058   \n",
              "2 -0.321707 -1.429819  12.251561  6.586874 -5.304647 -11.311090  17.812850   \n",
              "3 -0.245594  5.076677 -24.149632  3.637307  6.505811   2.290224 -35.111751   \n",
              "4 -0.273366  0.306326 -11.352593  1.676758  2.928441  -0.616824 -16.505817   \n",
              "\n",
              "          x7        x8        x9  ...        x41       x42       x43  \\\n",
              "0 -14.789997 -1.040673 -4.204950  ...  -1.497117  5.414063 -2.325655   \n",
              "1  -6.725709  0.896421  0.330165  ...  36.292790  4.490915  0.762561   \n",
              "2  11.060572  5.325880 -2.632984  ...  -0.368491  9.088864 -0.689886   \n",
              "3 -18.913592 -0.337041 -5.568076  ...  15.691546 -7.467775  2.940789   \n",
              "4  27.532281  1.199715 -4.309105  ... -13.911297 -5.229937  1.783928   \n",
              "\n",
              "        x44       x45        x46       x47       x48        x49  y  \n",
              "0  1.674827 -0.264332  60.781427 -7.689696  0.151589  -8.040166  0  \n",
              "1  6.526662  1.007927  15.805696 -4.896678 -0.320283  16.719974  0  \n",
              "2 -2.731118  0.754200  30.856417 -7.428573 -2.090804  -7.869421  0  \n",
              "3 -6.424112  0.419776 -72.424569  5.361375  1.806070  -7.670847  0  \n",
              "4  3.957801 -0.096988 -14.085435 -0.208351 -0.894942  15.724742  1  \n",
              "\n",
              "[5 rows x 48 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-26b3f223-3873-4b05-8ce8-0ba61cfae085\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x0</th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>x3</th>\n",
              "      <th>x4</th>\n",
              "      <th>x5</th>\n",
              "      <th>x6</th>\n",
              "      <th>x7</th>\n",
              "      <th>x8</th>\n",
              "      <th>x9</th>\n",
              "      <th>...</th>\n",
              "      <th>x41</th>\n",
              "      <th>x42</th>\n",
              "      <th>x43</th>\n",
              "      <th>x44</th>\n",
              "      <th>x45</th>\n",
              "      <th>x46</th>\n",
              "      <th>x47</th>\n",
              "      <th>x48</th>\n",
              "      <th>x49</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.166563</td>\n",
              "      <td>-3.961588</td>\n",
              "      <td>4.621113</td>\n",
              "      <td>2.481908</td>\n",
              "      <td>-1.800135</td>\n",
              "      <td>0.804684</td>\n",
              "      <td>6.718751</td>\n",
              "      <td>-14.789997</td>\n",
              "      <td>-1.040673</td>\n",
              "      <td>-4.204950</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.497117</td>\n",
              "      <td>5.414063</td>\n",
              "      <td>-2.325655</td>\n",
              "      <td>1.674827</td>\n",
              "      <td>-0.264332</td>\n",
              "      <td>60.781427</td>\n",
              "      <td>-7.689696</td>\n",
              "      <td>0.151589</td>\n",
              "      <td>-8.040166</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.149894</td>\n",
              "      <td>-0.585676</td>\n",
              "      <td>27.839856</td>\n",
              "      <td>4.152333</td>\n",
              "      <td>6.426802</td>\n",
              "      <td>-2.426943</td>\n",
              "      <td>40.477058</td>\n",
              "      <td>-6.725709</td>\n",
              "      <td>0.896421</td>\n",
              "      <td>0.330165</td>\n",
              "      <td>...</td>\n",
              "      <td>36.292790</td>\n",
              "      <td>4.490915</td>\n",
              "      <td>0.762561</td>\n",
              "      <td>6.526662</td>\n",
              "      <td>1.007927</td>\n",
              "      <td>15.805696</td>\n",
              "      <td>-4.896678</td>\n",
              "      <td>-0.320283</td>\n",
              "      <td>16.719974</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.321707</td>\n",
              "      <td>-1.429819</td>\n",
              "      <td>12.251561</td>\n",
              "      <td>6.586874</td>\n",
              "      <td>-5.304647</td>\n",
              "      <td>-11.311090</td>\n",
              "      <td>17.812850</td>\n",
              "      <td>11.060572</td>\n",
              "      <td>5.325880</td>\n",
              "      <td>-2.632984</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.368491</td>\n",
              "      <td>9.088864</td>\n",
              "      <td>-0.689886</td>\n",
              "      <td>-2.731118</td>\n",
              "      <td>0.754200</td>\n",
              "      <td>30.856417</td>\n",
              "      <td>-7.428573</td>\n",
              "      <td>-2.090804</td>\n",
              "      <td>-7.869421</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.245594</td>\n",
              "      <td>5.076677</td>\n",
              "      <td>-24.149632</td>\n",
              "      <td>3.637307</td>\n",
              "      <td>6.505811</td>\n",
              "      <td>2.290224</td>\n",
              "      <td>-35.111751</td>\n",
              "      <td>-18.913592</td>\n",
              "      <td>-0.337041</td>\n",
              "      <td>-5.568076</td>\n",
              "      <td>...</td>\n",
              "      <td>15.691546</td>\n",
              "      <td>-7.467775</td>\n",
              "      <td>2.940789</td>\n",
              "      <td>-6.424112</td>\n",
              "      <td>0.419776</td>\n",
              "      <td>-72.424569</td>\n",
              "      <td>5.361375</td>\n",
              "      <td>1.806070</td>\n",
              "      <td>-7.670847</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.273366</td>\n",
              "      <td>0.306326</td>\n",
              "      <td>-11.352593</td>\n",
              "      <td>1.676758</td>\n",
              "      <td>2.928441</td>\n",
              "      <td>-0.616824</td>\n",
              "      <td>-16.505817</td>\n",
              "      <td>27.532281</td>\n",
              "      <td>1.199715</td>\n",
              "      <td>-4.309105</td>\n",
              "      <td>...</td>\n",
              "      <td>-13.911297</td>\n",
              "      <td>-5.229937</td>\n",
              "      <td>1.783928</td>\n",
              "      <td>3.957801</td>\n",
              "      <td>-0.096988</td>\n",
              "      <td>-14.085435</td>\n",
              "      <td>-0.208351</td>\n",
              "      <td>-0.894942</td>\n",
              "      <td>15.724742</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 48 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26b3f223-3873-4b05-8ce8-0ba61cfae085')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-26b3f223-3873-4b05-8ce8-0ba61cfae085 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-26b3f223-3873-4b05-8ce8-0ba61cfae085');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frames = [cat2,numeric]\n",
        "\n",
        "df2 = pd.concat(frames)\n",
        "\n",
        "df2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN1Js-IXgRfY",
        "outputId": "046fe65c-60d0-4f57-f2af-e86f611e9af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(320000, 71)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqr9wJE5k6B9",
        "outputId": "540a9c3d-2217-47ab-f659-0637ba00a26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 320000 entries, 0 to 159999\n",
            "Data columns (total 71 columns):\n",
            " #   Column  Non-Null Count   Dtype             \n",
            "---  ------  --------------   -----             \n",
            " 0   0       160000 non-null  Sparse[float64, 0]\n",
            " 1   1       160000 non-null  Sparse[float64, 0]\n",
            " 2   2       160000 non-null  Sparse[float64, 0]\n",
            " 3   3       160000 non-null  Sparse[float64, 0]\n",
            " 4   4       160000 non-null  Sparse[float64, 0]\n",
            " 5   5       160000 non-null  Sparse[float64, 0]\n",
            " 6   6       160000 non-null  Sparse[float64, 0]\n",
            " 7   7       160000 non-null  Sparse[float64, 0]\n",
            " 8   8       160000 non-null  Sparse[float64, 0]\n",
            " 9   9       160000 non-null  Sparse[float64, 0]\n",
            " 10  10      160000 non-null  Sparse[float64, 0]\n",
            " 11  11      160000 non-null  Sparse[float64, 0]\n",
            " 12  12      160000 non-null  Sparse[float64, 0]\n",
            " 13  13      160000 non-null  Sparse[float64, 0]\n",
            " 14  14      160000 non-null  Sparse[float64, 0]\n",
            " 15  15      160000 non-null  Sparse[float64, 0]\n",
            " 16  16      160000 non-null  Sparse[float64, 0]\n",
            " 17  17      160000 non-null  Sparse[float64, 0]\n",
            " 18  18      160000 non-null  Sparse[float64, 0]\n",
            " 19  19      160000 non-null  Sparse[float64, 0]\n",
            " 20  20      160000 non-null  Sparse[float64, 0]\n",
            " 21  21      160000 non-null  Sparse[float64, 0]\n",
            " 22  22      160000 non-null  Sparse[float64, 0]\n",
            " 23  x0      160000 non-null  float64           \n",
            " 24  x1      160000 non-null  float64           \n",
            " 25  x10     160000 non-null  float64           \n",
            " 26  x11     160000 non-null  float64           \n",
            " 27  x12     160000 non-null  float64           \n",
            " 28  x13     160000 non-null  float64           \n",
            " 29  x14     160000 non-null  float64           \n",
            " 30  x15     160000 non-null  float64           \n",
            " 31  x16     160000 non-null  float64           \n",
            " 32  x17     160000 non-null  float64           \n",
            " 33  x18     160000 non-null  float64           \n",
            " 34  x19     160000 non-null  float64           \n",
            " 35  x2      160000 non-null  float64           \n",
            " 36  x20     160000 non-null  float64           \n",
            " 37  x21     160000 non-null  float64           \n",
            " 38  x22     160000 non-null  float64           \n",
            " 39  x23     160000 non-null  float64           \n",
            " 40  x25     160000 non-null  float64           \n",
            " 41  x26     160000 non-null  float64           \n",
            " 42  x27     160000 non-null  float64           \n",
            " 43  x28     160000 non-null  float64           \n",
            " 44  x3      160000 non-null  float64           \n",
            " 45  x31     160000 non-null  float64           \n",
            " 46  x32     160000 non-null  float64           \n",
            " 47  x33     160000 non-null  float64           \n",
            " 48  x34     160000 non-null  float64           \n",
            " 49  x35     160000 non-null  float64           \n",
            " 50  x36     160000 non-null  float64           \n",
            " 51  x37     160000 non-null  float64           \n",
            " 52  x38     160000 non-null  float64           \n",
            " 53  x39     160000 non-null  float64           \n",
            " 54  x4      160000 non-null  float64           \n",
            " 55  x40     160000 non-null  float64           \n",
            " 56  x41     160000 non-null  float64           \n",
            " 57  x42     160000 non-null  float64           \n",
            " 58  x43     160000 non-null  float64           \n",
            " 59  x44     160000 non-null  float64           \n",
            " 60  x45     160000 non-null  float64           \n",
            " 61  x46     160000 non-null  float64           \n",
            " 62  x47     160000 non-null  float64           \n",
            " 63  x48     160000 non-null  float64           \n",
            " 64  x49     160000 non-null  float64           \n",
            " 65  x5      160000 non-null  float64           \n",
            " 66  x6      160000 non-null  float64           \n",
            " 67  x7      160000 non-null  float64           \n",
            " 68  x8      160000 non-null  float64           \n",
            " 69  x9      160000 non-null  float64           \n",
            " 70  y       160000 non-null  float64           \n",
            "dtypes: Sparse[float64, 0](23), float64(48)\n",
            "memory usage: 167.2 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Count values in label\n",
        "df2['y'].value_counts()"
      ],
      "metadata": {
        "id": "jQOdmdBGjkX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5becc8-2190-41a7-9093-d47f9a0cbcac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    95803\n",
              "1.0    64197\n",
              "Name: y, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#class 1 misclassification is $100 penalty, class 0 misclassification is $20 penalty\n",
        "#Should downsample for more accuracy\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "labels = [\"0 = Class 0\", \"1 = Class 1\"]\n",
        "ax.bar(labels,df2[\"y\"].value_counts())\n",
        "plt.ylabel(\"counts\")\n",
        "plt.title('Counts of Classes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vNxigaR7z_Ws",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "30b46244-ab9d-4a0c-f262-8f093e454508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#code to downsample class 1\n",
        "# Separate majority and minority classes\n",
        "from sklearn.utils import resample\n",
        "\n",
        "df_class0 = df2[df2['y']==0]\n",
        "df_class1 = df2[df2['y']==1]\n",
        " \n",
        "# Downsample majority class\n",
        "class0_downsampled = resample(df_class0, \n",
        "                                 replace=False,    # sample without replacement\n",
        "                                 n_samples=64197,     # to match minority class\n",
        "                                 random_state=444) # reproducible results\n",
        " \n",
        "# Combine minority class with downsampled majority class\n",
        "df_downsampled = pd.concat([class0_downsampled, df_class1])\n",
        " \n",
        "# Display new class counts\n",
        "df_downsampled['y'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwqp69AC7Fob",
        "outputId": "2b98f108-2a82-47bf-ea17-2aaa1fb15510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    64197\n",
              "1.0    64197\n",
              "Name: y, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#class 1 misclassification is $100 penalty, class 0 misclassification is $20 penalty\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "labels = [\"0 = Class 0\", \"1 = Class 1\"]\n",
        "ax.bar(labels,df_downsampled[\"y\"].value_counts())\n",
        "plt.ylabel(\"counts\")\n",
        "plt.title('Counts of Classes')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "kt1EAQf4FBhc",
        "outputId": "6ce09848-4e21-47fb-963e-61ea98f679cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.relataly.com/using-random-search-to-tune-the-hyperparameters-of-a-random-decision-forest-with-python/6875/#h-step-2-explore-the-data"
      ],
      "metadata": {
        "id": "IfkdxOcuYZgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=df_downsampled.loc[:,df_downsampled.columns !='y'] # Features\n",
        "y=df_downsampled['y']  # Labels"
      ],
      "metadata": {
        "id": "jtlyln-3v6hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.fillna(0)"
      ],
      "metadata": {
        "id": "T4_uOxvXl-mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=444)"
      ],
      "metadata": {
        "id": "8T4uree7wTSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Estimator and the Parameter Ranges\n",
        "rf = RandomForestClassifier()\n",
        "n_estimators = [50, 100, 200]\n",
        "criterion=['gini', 'entropy', 'log_loss']\n",
        "min_samples_split = [5, 10, 20, 50]\n",
        "max_features = ['log2', 'sqrt']\n",
        "number_of_iterations = 20\n",
        "\n",
        "\n",
        "# Define the param distribution dictionary\n",
        "param_distributions = dict(n_estimators = n_estimators,\n",
        "                           criterion = criterion, \n",
        "                           min_samples_split=min_samples_split,\n",
        "                           max_features=max_features)\n"
      ],
      "metadata": {
        "id": "RSoIIya_tWPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the gridsearch\n",
        "grid = RandomizedSearchCV(estimator=rf, \n",
        "                          param_distributions=param_distributions, \n",
        "                          n_iter=number_of_iterations)\n"
      ],
      "metadata": {
        "id": "X50lZvoStY1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_results = grid.fit(X_train, y_train)\n",
        "\n",
        "# Summarize the results in a readable format\n",
        "print(\"Best params: {0}, using {1}\".format(grid_results.cv_results_['mean_test_score'], grid_results.best_params_))\n",
        "results_df = pd.DataFrame(grid_results.cv_results_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtwYVYqdzdWi",
        "outputId": "7c8b9dc4-d9aa-4595-9f07-5d2ade420d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "45 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_forest.py\", line 450, in fit\n",
            "    trees = Parallel(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 1085, in __call__\n",
            "    if self.dispatch_one_batch(iterator):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 901, in dispatch_one_batch\n",
            "    self._dispatch(tasks)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 819, in _dispatch\n",
            "    job = self._backend.apply_async(batch, callback=cb)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
            "    result = ImmediateResult(func)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/joblib/_parallel_backends.py\", line 597, in __init__\n",
            "    self.results = batch()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 288, in __call__\n",
            "    return [func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
            "    return [func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_forest.py\", line 185, in _parallel_build_trees\n",
            "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/tree/_classes.py\", line 937, in fit\n",
            "    super().fit(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/tree/_classes.py\", line 352, in fit\n",
            "    criterion = CRITERIA_CLF[self.criterion](\n",
            "KeyError: 'log_loss'\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.90708299        nan        nan 0.91134926        nan 0.9015612\n",
            " 0.89984072 0.90422329 0.91016355 0.90661797        nan        nan\n",
            "        nan        nan 0.90038709 0.90758288 0.9077921  0.91107028\n",
            "        nan        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: [0.90708299        nan        nan 0.91134926        nan 0.9015612\n",
            " 0.89984072 0.90422329 0.91016355 0.90661797        nan        nan\n",
            "        nan        nan 0.90038709 0.90758288 0.9077921  0.91107028\n",
            "        nan        nan], using {'n_estimators': 100, 'min_samples_split': 5, 'max_features': 'sqrt', 'criterion': 'entropy'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = grid_results.best_estimator_\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n7ErX-t1Ygz",
        "outputId": "5d570e83-6d5c-4b49-b823-78e868f53e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification Report\n",
        "\n",
        "target_names = ['class 0', 'class 1']\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHOBkRTk_ucz",
        "outputId": "9e77660b-6ee0-4f8b-df6e-e9906867d289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.92      0.92      0.92     21336\n",
            "     class 1       0.92      0.92      0.92     21035\n",
            "\n",
            "    accuracy                           0.92     42371\n",
            "   macro avg       0.92      0.92      0.92     42371\n",
            "weighted avg       0.92      0.92      0.92     42371\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
        "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
        " \n",
        "plt.xlabel('Predictions', fontsize=18)\n",
        "plt.ylabel('Actuals', fontsize=18)\n",
        "plt.title('Confusion Matrix', fontsize=18)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "bIqm56W1-zov",
        "outputId": "0880e9d5-bc85-4afa-bf3d-cf09bb5952e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 540x540 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "https://www.geeksforgeeks.org/hyperparameter-tuning-using-gridsearchcv-and-kerasclassifier/"
      ],
      "metadata": {
        "id": "uNsDIXcwFJow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_clf(unit):\n",
        "  # creating the layers of the NN\n",
        "  ann = tf.keras.models.Sequential()\n",
        "  ann.add(tf.keras.layers.Dense(units=unit, activation='tanh'))\n",
        "  ann.add(tf.keras.layers.Dense(units=unit, activation='relu'))\n",
        "  ann.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
        "  ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "  return ann\n"
      ],
      "metadata": {
        "id": "_bxTqSvOYIpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=KerasClassifier(build_fn=build_clf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU7tZpB5YT3t",
        "outputId": "df037d1c-a892-48a4-cb1f-45671d37f9e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-94990c6d4461>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model=KerasClassifier(build_fn=build_clf)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params={'batch_size':[100, 20, 50, 25, 32], \n",
        "        'nb_epoch':[200, 100, 300, 400],\n",
        "        'unit':[5,6, 10, 11, 12, 15],\n",
        "           \n",
        "        }\n",
        "gs=GridSearchCV(estimator=model, param_grid=params, cv=10)\n",
        "# now fit the dataset to the GridSearchCV object. \n",
        "fit = gs.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcF8CjX6YaKQ",
        "outputId": "67a526d1-07cf-4f91-a9f3-5ce02a968e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "775/775 [==============================] - 5s 3ms/step - loss: 4.3466 - accuracy: 0.4939\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.3537 - accuracy: 0.4954\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0226 - accuracy: 0.5254\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8524 - accuracy: 0.5256\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 3.7399 - accuracy: 0.5081\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.9437 - accuracy: 0.5081\n",
            "775/775 [==============================] - 4s 3ms/step - loss: 1.2678 - accuracy: 0.5123\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.9006 - accuracy: 0.5043\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.4361 - accuracy: 0.4967\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 1.5412 - accuracy: 0.4881\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.2969 - accuracy: 0.5308\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1987 - accuracy: 0.5993\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.3380 - accuracy: 0.4935\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.3022 - accuracy: 0.5024\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 7.0354 - accuracy: 0.5187\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.4615 - accuracy: 0.5494\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.3219 - accuracy: 0.5373\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.0712 - accuracy: 0.5681\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0507 - accuracy: 0.5168\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7316 - accuracy: 0.5485\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9337 - accuracy: 0.5266\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.6738 - accuracy: 0.5953\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8217 - accuracy: 0.5027\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7823 - accuracy: 0.5056\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.4478 - accuracy: 0.6042\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.6816 - accuracy: 0.6464\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.5307 - accuracy: 0.5107\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1765 - accuracy: 0.5084\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.2974 - accuracy: 0.5612\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7792 - accuracy: 0.6278\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9728 - accuracy: 0.5122\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8261 - accuracy: 0.5124\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 7.7301 - accuracy: 0.4986\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 7.7595 - accuracy: 0.4955\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.6830 - accuracy: 0.5000\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.2282 - accuracy: 0.5015\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.6058 - accuracy: 0.5609\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8474 - accuracy: 0.6136\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.4858 - accuracy: 0.5112\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8586 - accuracy: 0.4886\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.5163 - accuracy: 0.5431\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.2672 - accuracy: 0.5661\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.6940 - accuracy: 0.5331\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7983 - accuracy: 0.5617\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.5840 - accuracy: 0.5247\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7580 - accuracy: 0.5871\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9938 - accuracy: 0.5307\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8346 - accuracy: 0.5419\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8164 - accuracy: 0.5039\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7501 - accuracy: 0.5041\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.4650 - accuracy: 0.5272\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7696 - accuracy: 0.5425\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 7.7347 - accuracy: 0.4986\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 7.7824 - accuracy: 0.4955\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.7578 - accuracy: 0.5233\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.9260 - accuracy: 0.5398\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 3.7165 - accuracy: 0.5250\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8433 - accuracy: 0.5280\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8286 - accuracy: 0.5535\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7194 - accuracy: 0.6171\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8074 - accuracy: 0.5280\n",
            "87/87 [==============================] - 0s 3ms/step - loss: 0.7424 - accuracy: 0.5875\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.4651 - accuracy: 0.5370\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7128 - accuracy: 0.5681\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.1164 - accuracy: 0.5253\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8650 - accuracy: 0.5221\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.9624 - accuracy: 0.5005\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.9296 - accuracy: 0.4955\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.1953 - accuracy: 0.5205\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8042 - accuracy: 0.5189\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.2317 - accuracy: 0.5241\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8856 - accuracy: 0.5334\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.6990 - accuracy: 0.5164\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 1.0077 - accuracy: 0.5570\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 3.2953 - accuracy: 0.5139\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7038 - accuracy: 0.5771\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.1199 - accuracy: 0.5205\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7906 - accuracy: 0.5152\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.1559 - accuracy: 0.5166\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8218 - accuracy: 0.5328\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0093 - accuracy: 0.5536\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7299 - accuracy: 0.6092\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.1495 - accuracy: 0.5209\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7602 - accuracy: 0.5262\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.1364 - accuracy: 0.5628\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.6723 - accuracy: 0.5926\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.5525 - accuracy: 0.5520\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8693 - accuracy: 0.6196\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8296 - accuracy: 0.5135\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7782 - accuracy: 0.5101\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.8331 - accuracy: 0.4925\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8773 - accuracy: 0.4997\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8457 - accuracy: 0.5302\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7419 - accuracy: 0.5384\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.3452 - accuracy: 0.5094\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8234 - accuracy: 0.5112\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9415 - accuracy: 0.5361\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7487 - accuracy: 0.5921\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9136 - accuracy: 0.5712\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7424 - accuracy: 0.6025\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.7084 - accuracy: 0.5145\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8873 - accuracy: 0.5311\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.3416 - accuracy: 0.5272\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8548 - accuracy: 0.5245\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.3882 - accuracy: 0.5159\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8539 - accuracy: 0.5088\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9790 - accuracy: 0.5299\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7411 - accuracy: 0.5949\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0930 - accuracy: 0.5083\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8338 - accuracy: 0.5561\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.8008 - accuracy: 0.5341\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8472 - accuracy: 0.5495\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0745 - accuracy: 0.5867\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.6990 - accuracy: 0.6528\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9521 - accuracy: 0.5931\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7047 - accuracy: 0.6628\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.8577 - accuracy: 0.5072\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7891 - accuracy: 0.5258\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.0018 - accuracy: 0.5216\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 3.7747 - accuracy: 0.5335\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.2073 - accuracy: 0.5139\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1106 - accuracy: 0.5206\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.6486 - accuracy: 0.5319\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.2611 - accuracy: 0.5406\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.3685 - accuracy: 0.5154\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8079 - accuracy: 0.5333\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 3.0436 - accuracy: 0.5123\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7894 - accuracy: 0.4921\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.4822 - accuracy: 0.4988\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8199 - accuracy: 0.5019\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.1456 - accuracy: 0.5555\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.6640 - accuracy: 0.6074\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.0798 - accuracy: 0.5027\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7174 - accuracy: 0.5165\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0166 - accuracy: 0.5091\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7457 - accuracy: 0.5231\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.4449 - accuracy: 0.5104\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.9474 - accuracy: 0.5200\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.2933 - accuracy: 0.5168\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 1.0652 - accuracy: 0.4843\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 7.2074 - accuracy: 0.5020\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.0350 - accuracy: 0.5528\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.3829 - accuracy: 0.4941\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8447 - accuracy: 0.4972\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.4396 - accuracy: 0.4898\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.2876 - accuracy: 0.4933\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.1339 - accuracy: 0.5125\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 3.9635 - accuracy: 0.5206\n",
            "775/775 [==============================] - 3s 4ms/step - loss: 1.3198 - accuracy: 0.5100\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.9800 - accuracy: 0.5064\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.2016 - accuracy: 0.5129\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8253 - accuracy: 0.5145\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0669 - accuracy: 0.5879\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7186 - accuracy: 0.6167\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 7.7377 - accuracy: 0.4984\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 7.7555 - accuracy: 0.4972\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.4222 - accuracy: 0.5017\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7255 - accuracy: 0.5220\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9477 - accuracy: 0.5073\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8018 - accuracy: 0.5271\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.8874 - accuracy: 0.5179\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7917 - accuracy: 0.5270\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.1190 - accuracy: 0.5108\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7730 - accuracy: 0.5119\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 3.7992 - accuracy: 0.5202\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7947 - accuracy: 0.5167\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 5.7312 - accuracy: 0.5227\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.2904 - accuracy: 0.5759\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.7159 - accuracy: 0.5460\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8242 - accuracy: 0.5736\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 5.5751 - accuracy: 0.5073\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.9757 - accuracy: 0.5160\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8522 - accuracy: 0.4970\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7612 - accuracy: 0.5016\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.2609 - accuracy: 0.5296\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7621 - accuracy: 0.5362\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.7084 - accuracy: 0.5112\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8422 - accuracy: 0.5294\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8874 - accuracy: 0.5608\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.6801 - accuracy: 0.6142\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 7.6308 - accuracy: 0.4981\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 7.6501 - accuracy: 0.4935\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 5.7877 - accuracy: 0.5210\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1337 - accuracy: 0.5209\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.6601 - accuracy: 0.5044\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.9836 - accuracy: 0.5060\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 5.6649 - accuracy: 0.5026\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1122 - accuracy: 0.5173\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.2049 - accuracy: 0.5418\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8673 - accuracy: 0.5489\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.7734 - accuracy: 0.5478\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7270 - accuracy: 0.5665\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.3005 - accuracy: 0.5965\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7894 - accuracy: 0.6608\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.4415 - accuracy: 0.5194\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8846 - accuracy: 0.5276\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.1590 - accuracy: 0.5251\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7200 - accuracy: 0.5559\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.1501 - accuracy: 0.5147\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7442 - accuracy: 0.5329\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.3157 - accuracy: 0.5176\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.2733 - accuracy: 0.5521\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8433 - accuracy: 0.5063\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7807 - accuracy: 0.5206\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8432 - accuracy: 0.5173\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7678 - accuracy: 0.5320\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9640 - accuracy: 0.5254\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8016 - accuracy: 0.5294\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9622 - accuracy: 0.5150\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7971 - accuracy: 0.5321\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.5747 - accuracy: 0.5190\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8081 - accuracy: 0.5160\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 6.4679 - accuracy: 0.5089\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1244 - accuracy: 0.5323\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8823 - accuracy: 0.5309\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7065 - accuracy: 0.5259\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.6095 - accuracy: 0.4952\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.2068 - accuracy: 0.4988\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0635 - accuracy: 0.5701\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7382 - accuracy: 0.5964\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.1888 - accuracy: 0.5191\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7301 - accuracy: 0.5305\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.3401 - accuracy: 0.5439\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7337 - accuracy: 0.5699\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.7818 - accuracy: 0.5381\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.6761 - accuracy: 0.6086\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9191 - accuracy: 0.5266\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7944 - accuracy: 0.5272\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.2118 - accuracy: 0.5443\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7955 - accuracy: 0.5728\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.1293 - accuracy: 0.5262\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7832 - accuracy: 0.5343\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.5283 - accuracy: 0.5039\n",
            "87/87 [==============================] - 0s 3ms/step - loss: 0.8451 - accuracy: 0.4926\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9428 - accuracy: 0.5840\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7916 - accuracy: 0.6079\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.4007 - accuracy: 0.5565\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7921 - accuracy: 0.6106\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.3460 - accuracy: 0.5350\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 1.0206 - accuracy: 0.5607\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.2715 - accuracy: 0.4972\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1688 - accuracy: 0.4998\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 6.2444 - accuracy: 0.4962\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1594 - accuracy: 0.4981\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 5.2532 - accuracy: 0.4941\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.2744 - accuracy: 0.4897\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 5.2787 - accuracy: 0.5193\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 2.7548 - accuracy: 0.5307\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.3771 - accuracy: 0.4944\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1596 - accuracy: 0.4941\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.3233 - accuracy: 0.4966\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.2870 - accuracy: 0.4930\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.3633 - accuracy: 0.5357\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7080 - accuracy: 0.5611\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 6.1381 - accuracy: 0.4969\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1984 - accuracy: 0.4965\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.2702 - accuracy: 0.5608\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7521 - accuracy: 0.6109\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.3122 - accuracy: 0.5000\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.3335 - accuracy: 0.4849\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.1518 - accuracy: 0.5146\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1026 - accuracy: 0.5183\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9422 - accuracy: 0.5033\n",
            "87/87 [==============================] - 0s 3ms/step - loss: 0.8630 - accuracy: 0.5180\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 6.0475 - accuracy: 0.5073\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.7457 - accuracy: 0.5216\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.3005 - accuracy: 0.4977\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7746 - accuracy: 0.5239\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.3615 - accuracy: 0.4985\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1863 - accuracy: 0.5049\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 7.7560 - accuracy: 0.4972\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 7.5905 - accuracy: 0.5079\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.7403 - accuracy: 0.4908\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 1.0422 - accuracy: 0.5066\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0402 - accuracy: 0.5225\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7673 - accuracy: 0.5313\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8327 - accuracy: 0.5174\n",
            "87/87 [==============================] - 0s 3ms/step - loss: 0.7637 - accuracy: 0.5133\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.7425 - accuracy: 0.5026\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.3645 - accuracy: 0.5050\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8278 - accuracy: 0.5141\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7750 - accuracy: 0.5261\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 7.7336 - accuracy: 0.4983\n",
            "87/87 [==============================] - 0s 3ms/step - loss: 7.6396 - accuracy: 0.4983\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.4442 - accuracy: 0.5168\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.9283 - accuracy: 0.5331\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.9649 - accuracy: 0.5167\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7819 - accuracy: 0.5157\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.4174 - accuracy: 0.4921\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.9492 - accuracy: 0.4964\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 3.6641 - accuracy: 0.5027\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8777 - accuracy: 0.5155\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.7838 - accuracy: 0.5424\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.6892 - accuracy: 0.5771\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.2644 - accuracy: 0.5202\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.2296 - accuracy: 0.5260\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8952 - accuracy: 0.5682\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7463 - accuracy: 0.6020\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.4678 - accuracy: 0.5215\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 1.0310 - accuracy: 0.5250\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.4691 - accuracy: 0.5399\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7812 - accuracy: 0.5674\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0464 - accuracy: 0.5126\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7678 - accuracy: 0.5097\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.2734 - accuracy: 0.5571\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1460 - accuracy: 0.6021\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.8231 - accuracy: 0.5037\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8124 - accuracy: 0.5107\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.5871 - accuracy: 0.5172\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 1.0879 - accuracy: 0.5265\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.9220 - accuracy: 0.5081\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7675 - accuracy: 0.5103\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 5.1640 - accuracy: 0.5199\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1807 - accuracy: 0.5267\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 5.6892 - accuracy: 0.4983\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 3.9663 - accuracy: 0.5050\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9159 - accuracy: 0.5707\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.6865 - accuracy: 0.6097\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.5160 - accuracy: 0.5764\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.6420 - accuracy: 0.6488\n",
            "775/775 [==============================] - 3s 4ms/step - loss: 1.1120 - accuracy: 0.5094\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7475 - accuracy: 0.5482\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.2933 - accuracy: 0.4941\n",
            "87/87 [==============================] - 1s 2ms/step - loss: 1.0185 - accuracy: 0.5110\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.3139 - accuracy: 0.5714\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1348 - accuracy: 0.5928\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9450 - accuracy: 0.5221\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7726 - accuracy: 0.6317\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.3110 - accuracy: 0.5453\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7248 - accuracy: 0.5663\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8632 - accuracy: 0.5959\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.6751 - accuracy: 0.6501\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.6634 - accuracy: 0.5384\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8174 - accuracy: 0.6052\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.3200 - accuracy: 0.5051\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.2287 - accuracy: 0.5191\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8441 - accuracy: 0.5336\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7489 - accuracy: 0.5672\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.8952 - accuracy: 0.5420\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7786 - accuracy: 0.5635\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9368 - accuracy: 0.5511\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7793 - accuracy: 0.5562\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.1155 - accuracy: 0.5448\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8267 - accuracy: 0.5936\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0722 - accuracy: 0.5030\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7768 - accuracy: 0.5202\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.6335 - accuracy: 0.5357\n",
            "87/87 [==============================] - 1s 2ms/step - loss: 0.8271 - accuracy: 0.5721\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.2994 - accuracy: 0.5104\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8171 - accuracy: 0.5214\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.6849 - accuracy: 0.5093\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7468 - accuracy: 0.5570\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.4063 - accuracy: 0.5200\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8837 - accuracy: 0.5904\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 5.5643 - accuracy: 0.5376\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.2629 - accuracy: 0.5977\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.6897 - accuracy: 0.5237\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8321 - accuracy: 0.5128\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.4819 - accuracy: 0.5151\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8611 - accuracy: 0.5603\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 3.9704 - accuracy: 0.5144\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 3.5442 - accuracy: 0.5249\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0201 - accuracy: 0.4992\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.9505 - accuracy: 0.5038\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.1512 - accuracy: 0.5251\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.0164 - accuracy: 0.5156\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.0830 - accuracy: 0.5090\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 3.9644 - accuracy: 0.5138\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9956 - accuracy: 0.4967\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8029 - accuracy: 0.5028\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 3.8372 - accuracy: 0.4990\n",
            "87/87 [==============================] - 1s 3ms/step - loss: 0.7877 - accuracy: 0.4914\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.4804 - accuracy: 0.4994\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1390 - accuracy: 0.5071\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.4360 - accuracy: 0.5010\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7915 - accuracy: 0.5121\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.1730 - accuracy: 0.5132\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.0177 - accuracy: 0.5350\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.6617 - accuracy: 0.5215\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 3.7087 - accuracy: 0.5481\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.5237 - accuracy: 0.5050\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.3090 - accuracy: 0.5235\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 7.6230 - accuracy: 0.4978\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 7.5290 - accuracy: 0.4982\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.2916 - accuracy: 0.5054\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7384 - accuracy: 0.5141\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.2882 - accuracy: 0.5333\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.0492 - accuracy: 0.5551\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 5.1351 - accuracy: 0.5020\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 1.5389 - accuracy: 0.4993\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.5364 - accuracy: 0.5091\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8628 - accuracy: 0.5230\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 6.0164 - accuracy: 0.4985\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.4676 - accuracy: 0.4953\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.2018 - accuracy: 0.5206\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.1927 - accuracy: 0.5251\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.8836 - accuracy: 0.4993\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7842 - accuracy: 0.5272\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.6450 - accuracy: 0.5107\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7022 - accuracy: 0.4892\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.7274 - accuracy: 0.5316\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.6957 - accuracy: 0.5559\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.0724 - accuracy: 0.5581\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7420 - accuracy: 0.5857\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.7109 - accuracy: 0.5375\n",
            "87/87 [==============================] - 0s 3ms/step - loss: 0.7485 - accuracy: 0.5370\n",
            "775/775 [==============================] - 3s 4ms/step - loss: 1.2803 - accuracy: 0.4985\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7652 - accuracy: 0.5041\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.4783 - accuracy: 0.5434\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 1.0235 - accuracy: 0.5692\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.2016 - accuracy: 0.5125\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7979 - accuracy: 0.5038\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.5509 - accuracy: 0.5131\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7793 - accuracy: 0.5205\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.2433 - accuracy: 0.5372\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7714 - accuracy: 0.5714\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.9955 - accuracy: 0.5327\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7506 - accuracy: 0.5548\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.2607 - accuracy: 0.5512\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8867 - accuracy: 0.5610\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0621 - accuracy: 0.5299\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8103 - accuracy: 0.5386\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0663 - accuracy: 0.5200\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8457 - accuracy: 0.5314\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.1000 - accuracy: 0.5134\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8060 - accuracy: 0.5180\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.9864 - accuracy: 0.5496\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8667 - accuracy: 0.5722\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.8669 - accuracy: 0.5270\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8227 - accuracy: 0.5528\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.3386 - accuracy: 0.5418\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.2373 - accuracy: 0.5834\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.8901 - accuracy: 0.5048\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8637 - accuracy: 0.5329\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.5241 - accuracy: 0.5393\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8213 - accuracy: 0.5592\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 3.3742 - accuracy: 0.5038\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.9071 - accuracy: 0.5210\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.3251 - accuracy: 0.5066\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7997 - accuracy: 0.5112\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 3.5110 - accuracy: 0.5183\n",
            "87/87 [==============================] - 0s 3ms/step - loss: 0.9572 - accuracy: 0.5132\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0856 - accuracy: 0.5358\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7898 - accuracy: 0.5742\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.2622 - accuracy: 0.5494\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7427 - accuracy: 0.6294\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.2662 - accuracy: 0.5321\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7688 - accuracy: 0.6032\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9619 - accuracy: 0.5020\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7768 - accuracy: 0.5230\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9360 - accuracy: 0.5269\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7249 - accuracy: 0.5324\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.0130 - accuracy: 0.5323\n",
            "87/87 [==============================] - 0s 3ms/step - loss: 1.0261 - accuracy: 0.5512\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.6584 - accuracy: 0.5349\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.9339 - accuracy: 0.5205\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.4166 - accuracy: 0.5366\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.0867 - accuracy: 0.5742\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 3.5584 - accuracy: 0.5120\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 1.0118 - accuracy: 0.5006\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.1852 - accuracy: 0.5251\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7526 - accuracy: 0.6105\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.3630 - accuracy: 0.5252\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7836 - accuracy: 0.5432\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0361 - accuracy: 0.5373\n",
            "87/87 [==============================] - 0s 3ms/step - loss: 0.7492 - accuracy: 0.6429\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0239 - accuracy: 0.5758\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8704 - accuracy: 0.6002\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 0.9651 - accuracy: 0.5268\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7924 - accuracy: 0.5335\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.0446 - accuracy: 0.5459\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.7972 - accuracy: 0.5661\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 2.6095 - accuracy: 0.5097\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8135 - accuracy: 0.5174\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.1027 - accuracy: 0.5576\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8179 - accuracy: 0.5780\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 4.6857 - accuracy: 0.5491\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 4.0586 - accuracy: 0.5809\n",
            "775/775 [==============================] - 3s 3ms/step - loss: 1.8453 - accuracy: 0.5074\n",
            "87/87 [==============================] - 0s 2ms/step - loss: 0.8142 - accuracy: 0.5299\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.6799 - accuracy: 0.5701\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6295 - accuracy: 0.6486\n",
            "3871/3871 [==============================] - 11s 3ms/step - loss: 2.2297 - accuracy: 0.5321\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7328 - accuracy: 0.5548\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 1.6511 - accuracy: 0.5142\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8176 - accuracy: 0.5278\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.3762 - accuracy: 0.5696\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7107 - accuracy: 0.6215\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 3.7724 - accuracy: 0.5003\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.9348 - accuracy: 0.5102\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 7.7553 - accuracy: 0.4972\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 7.5905 - accuracy: 0.5079\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 2.9833 - accuracy: 0.5346\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7815 - accuracy: 0.5334\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 3.0702 - accuracy: 0.5069\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.9575 - accuracy: 0.4987\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 4.2745 - accuracy: 0.4925\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7555 - accuracy: 0.5265\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.2066 - accuracy: 0.5187\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7240 - accuracy: 0.6285\n",
            "3871/3871 [==============================] - 11s 3ms/step - loss: 2.5620 - accuracy: 0.6176\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6946 - accuracy: 0.6889\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 5.1126 - accuracy: 0.5421\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 4.0468 - accuracy: 0.5891\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.9039 - accuracy: 0.5057\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7762 - accuracy: 0.5170\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.7365 - accuracy: 0.5844\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6723 - accuracy: 0.5987\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 7.7375 - accuracy: 0.4984\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 7.7573 - accuracy: 0.4971\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8024 - accuracy: 0.5099\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7151 - accuracy: 0.5331\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.9761 - accuracy: 0.5140\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7163 - accuracy: 0.5241\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.7828 - accuracy: 0.5092\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6961 - accuracy: 0.5180\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.9838 - accuracy: 0.5103\n",
            "431/431 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5277\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 7.7118 - accuracy: 0.5000\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 7.9886 - accuracy: 0.4821\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.7582 - accuracy: 0.5840\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6416 - accuracy: 0.6831\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 1.7992 - accuracy: 0.5737\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6394 - accuracy: 0.6806\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.9832 - accuracy: 0.5308\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7690 - accuracy: 0.5309\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8640 - accuracy: 0.5569\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6810 - accuracy: 0.6219\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.1373 - accuracy: 0.5504\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6889 - accuracy: 0.6364\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8037 - accuracy: 0.5754\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6845 - accuracy: 0.6138\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.4272 - accuracy: 0.5785\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7249 - accuracy: 0.6199\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 2.8715 - accuracy: 0.5625\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6996 - accuracy: 0.5513\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.9601 - accuracy: 0.6073\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6295 - accuracy: 0.6829\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.0272 - accuracy: 0.5185\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7396 - accuracy: 0.5310\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.8069 - accuracy: 0.5276\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7184 - accuracy: 0.5257\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 1.1353 - accuracy: 0.5627\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7926 - accuracy: 0.5350\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.8320 - accuracy: 0.5200\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6986 - accuracy: 0.5417\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.3956 - accuracy: 0.5866\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6410 - accuracy: 0.6525\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 2.4288 - accuracy: 0.5926\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6770 - accuracy: 0.6379\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.1043 - accuracy: 0.5193\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8067 - accuracy: 0.5212\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 3.2465 - accuracy: 0.5209\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8090 - accuracy: 0.5196\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.7002 - accuracy: 0.6185\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.5999 - accuracy: 0.7003\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.7281 - accuracy: 0.5623\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6483 - accuracy: 0.6478\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.9010 - accuracy: 0.5317\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6607 - accuracy: 0.6244\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.9220 - accuracy: 0.6813\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6310 - accuracy: 0.6505\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 1.5371 - accuracy: 0.5703\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7100 - accuracy: 0.6519\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 2.0979 - accuracy: 0.5277\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8035 - accuracy: 0.5638\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.5970 - accuracy: 0.5407\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7552 - accuracy: 0.5761\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.9824 - accuracy: 0.6024\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6359 - accuracy: 0.7202\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 2.5828 - accuracy: 0.5720\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7865 - accuracy: 0.6059\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8979 - accuracy: 0.6130\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6330 - accuracy: 0.6851\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.9282 - accuracy: 0.5339\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7322 - accuracy: 0.5559\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.7706 - accuracy: 0.5566\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6807 - accuracy: 0.5927\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8827 - accuracy: 0.5966\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6414 - accuracy: 0.6690\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 3.7319 - accuracy: 0.5738\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6281 - accuracy: 0.6850\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 1.0483 - accuracy: 0.5627\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6585 - accuracy: 0.6622\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.9474 - accuracy: 0.5637\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6593 - accuracy: 0.6466\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.0745 - accuracy: 0.5418\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7553 - accuracy: 0.5611\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 2.5448 - accuracy: 0.5946\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6384 - accuracy: 0.6737\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.7521 - accuracy: 0.6321\n",
            "431/431 [==============================] - 1s 3ms/step - loss: 0.5993 - accuracy: 0.6902\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.7288 - accuracy: 0.5999\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6239 - accuracy: 0.6891\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.2436 - accuracy: 0.5556\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6774 - accuracy: 0.6478\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.3092 - accuracy: 0.5416\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7968 - accuracy: 0.5652\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8024 - accuracy: 0.6121\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6709 - accuracy: 0.6683\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 1.5071 - accuracy: 0.5222\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7078 - accuracy: 0.5418\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.7422 - accuracy: 0.6203\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6949 - accuracy: 0.6606\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.9860 - accuracy: 0.5223\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7645 - accuracy: 0.5392\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8233 - accuracy: 0.5199\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7223 - accuracy: 0.5510\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.1969 - accuracy: 0.5116\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7962 - accuracy: 0.5114\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.4084 - accuracy: 0.5189\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7471 - accuracy: 0.5686\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.7562 - accuracy: 0.5282\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7121 - accuracy: 0.5377\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8243 - accuracy: 0.5346\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7611 - accuracy: 0.5478\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.7451 - accuracy: 0.5165\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6879 - accuracy: 0.5458\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.1435 - accuracy: 0.5202\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6693 - accuracy: 0.5793\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 3.3948 - accuracy: 0.5502\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8818 - accuracy: 0.6562\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 1.4814 - accuracy: 0.6143\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6298 - accuracy: 0.6751\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 1.8100 - accuracy: 0.5084\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7738 - accuracy: 0.5221\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 2.6831 - accuracy: 0.5252\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7865 - accuracy: 0.5107\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.7126 - accuracy: 0.5334\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6742 - accuracy: 0.5920\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.0111 - accuracy: 0.5180\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7079 - accuracy: 0.5957\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 7.7347 - accuracy: 0.4986\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 7.7824 - accuracy: 0.4955\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.2697 - accuracy: 0.5058\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8642 - accuracy: 0.4963\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8031 - accuracy: 0.5054\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7359 - accuracy: 0.5066\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.9086 - accuracy: 0.5455\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6574 - accuracy: 0.6047\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 2.3931 - accuracy: 0.5795\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7093 - accuracy: 0.6670\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.7213 - accuracy: 0.5820\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6414 - accuracy: 0.6394\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.7142 - accuracy: 0.5636\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6590 - accuracy: 0.5964\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 3.4789 - accuracy: 0.5165\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7846 - accuracy: 0.5192\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.3871 - accuracy: 0.5454\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7375 - accuracy: 0.5723\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.7594 - accuracy: 0.5109\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7711 - accuracy: 0.4921\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8385 - accuracy: 0.5600\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6433\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8046 - accuracy: 0.6053\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6796 - accuracy: 0.6486\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.7863 - accuracy: 0.5912\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6237 - accuracy: 0.7034\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8371 - accuracy: 0.5215\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7091 - accuracy: 0.5143\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.9093 - accuracy: 0.5352\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8753 - accuracy: 0.5123\n",
            "3871/3871 [==============================] - 11s 3ms/step - loss: 0.8176 - accuracy: 0.6201\n",
            "431/431 [==============================] - 1s 3ms/step - loss: 0.6894 - accuracy: 0.6576\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 0.8937 - accuracy: 0.5520\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7026 - accuracy: 0.5600\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 4.8355 - accuracy: 0.5108\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 4.0407 - accuracy: 0.5330\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.3987 - accuracy: 0.5203\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7524 - accuracy: 0.5140\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.8554 - accuracy: 0.5120\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7225 - accuracy: 0.5179\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.7222 - accuracy: 0.6312\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6233 - accuracy: 0.6954\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.7792 - accuracy: 0.5917\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7089 - accuracy: 0.6586\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.2640 - accuracy: 0.6525\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.5980 - accuracy: 0.7424\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.9918 - accuracy: 0.5118\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7190 - accuracy: 0.5155\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 1.1575 - accuracy: 0.5655\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6443 - accuracy: 0.6829\n",
            "3871/3871 [==============================] - 11s 3ms/step - loss: 0.7717 - accuracy: 0.6096\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6197 - accuracy: 0.6928\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.8189 - accuracy: 0.6125\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6320 - accuracy: 0.6714\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.8050 - accuracy: 0.5264\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7258 - accuracy: 0.5449\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.7970 - accuracy: 0.5759\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.5908 - accuracy: 0.7072\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.8131 - accuracy: 0.5659\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6757 - accuracy: 0.6576\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.0585 - accuracy: 0.5879\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8100 - accuracy: 0.6707\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.3471 - accuracy: 0.5703\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6555 - accuracy: 0.6352\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.9166 - accuracy: 0.5101\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7103 - accuracy: 0.5317\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 4.1606 - accuracy: 0.5199\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6914 - accuracy: 0.5981\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.8954 - accuracy: 0.5768\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6804 - accuracy: 0.6620\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 2.5164 - accuracy: 0.5571\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7457 - accuracy: 0.6119\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.9452 - accuracy: 0.5659\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7320 - accuracy: 0.5854\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.9569 - accuracy: 0.5990\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7453 - accuracy: 0.6360\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.0448 - accuracy: 0.6280\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6595 - accuracy: 0.6590\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 2.8252 - accuracy: 0.6089\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6327 - accuracy: 0.6882\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.9142 - accuracy: 0.5594\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6915 - accuracy: 0.6219\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 3.0865 - accuracy: 0.5550\n",
            "431/431 [==============================] - 1s 3ms/step - loss: 0.6818 - accuracy: 0.6690\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.5114 - accuracy: 0.6090\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6456 - accuracy: 0.6764\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.7731 - accuracy: 0.5707\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6609 - accuracy: 0.6552\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 1.9930 - accuracy: 0.5722\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7170 - accuracy: 0.6361\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 0.7166 - accuracy: 0.6118\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6309 - accuracy: 0.6957\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.7398 - accuracy: 0.5553\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6242 - accuracy: 0.6767\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8548 - accuracy: 0.5431\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6201 - accuracy: 0.6822\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8458 - accuracy: 0.5059\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7770 - accuracy: 0.4974\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.4477 - accuracy: 0.5238\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7886 - accuracy: 0.5165\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.8660 - accuracy: 0.5047\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7055 - accuracy: 0.5224\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.9061 - accuracy: 0.5156\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8421 - accuracy: 0.5210\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 3.9886 - accuracy: 0.5183\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8305 - accuracy: 0.5129\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.7874 - accuracy: 0.5030\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6994 - accuracy: 0.5293\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 1.1385 - accuracy: 0.5173\n",
            "431/431 [==============================] - 1s 3ms/step - loss: 0.7465 - accuracy: 0.5325\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 5.5090 - accuracy: 0.4982\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 4.1938 - accuracy: 0.5047\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 1.2255 - accuracy: 0.5187\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7014 - accuracy: 0.5177\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.0267 - accuracy: 0.5090\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7791 - accuracy: 0.5005\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.8692 - accuracy: 0.5812\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6605 - accuracy: 0.6533\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8242 - accuracy: 0.5024\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8292 - accuracy: 0.5095\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 3.3428 - accuracy: 0.5154\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6939 - accuracy: 0.5234\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.0624 - accuracy: 0.5083\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7807 - accuracy: 0.5028\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.9859 - accuracy: 0.5481\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6973 - accuracy: 0.6324\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.6569 - accuracy: 0.5096\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6947 - accuracy: 0.5181\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 0.8816 - accuracy: 0.5290\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7500 - accuracy: 0.5283\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 3.3063 - accuracy: 0.5825\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6363 - accuracy: 0.6936\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 1.7190 - accuracy: 0.6139\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6347 - accuracy: 0.6502\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.9818 - accuracy: 0.6089\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7316 - accuracy: 0.5987\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8152 - accuracy: 0.5767\n",
            "431/431 [==============================] - 1s 3ms/step - loss: 0.6821 - accuracy: 0.6002\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.5279 - accuracy: 0.5079\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7662 - accuracy: 0.5157\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.7721 - accuracy: 0.6147\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6429 - accuracy: 0.6561\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.4959 - accuracy: 0.5749\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6790 - accuracy: 0.6833\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 3.2467 - accuracy: 0.5779\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.9348 - accuracy: 0.6625\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.9981 - accuracy: 0.5847\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6682 - accuracy: 0.6866\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 1.2039 - accuracy: 0.5581\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7760 - accuracy: 0.5942\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 0.8762 - accuracy: 0.5970\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6204 - accuracy: 0.6458\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 0.8050 - accuracy: 0.5032\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7301 - accuracy: 0.4983\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8342 - accuracy: 0.5560\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7108 - accuracy: 0.5629\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 2.7615 - accuracy: 0.5988\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7139 - accuracy: 0.6343\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.3468 - accuracy: 0.5972\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6482 - accuracy: 0.6773\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 2.2402 - accuracy: 0.5294\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7183 - accuracy: 0.5666\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.9574 - accuracy: 0.5641\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7015 - accuracy: 0.5923\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.7891 - accuracy: 0.5551\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6202 - accuracy: 0.6637\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.2540 - accuracy: 0.5191\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7156 - accuracy: 0.5530\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.9859 - accuracy: 0.5824\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6915 - accuracy: 0.6529\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 0.7672 - accuracy: 0.5719\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6655 - accuracy: 0.6162\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 2.9713 - accuracy: 0.5828\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6867\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.9567 - accuracy: 0.5339\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7429 - accuracy: 0.5455\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.7522 - accuracy: 0.6381\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6157 - accuracy: 0.6841\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.7726 - accuracy: 0.5931\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6584 - accuracy: 0.6522\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.9383 - accuracy: 0.5606\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7028 - accuracy: 0.6585\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 2.3984 - accuracy: 0.5166\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8116 - accuracy: 0.5381\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.1689 - accuracy: 0.6127\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6382 - accuracy: 0.6626\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.2209 - accuracy: 0.5310\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7757 - accuracy: 0.5353\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 1.1459 - accuracy: 0.5429\n",
            "431/431 [==============================] - 1s 3ms/step - loss: 0.6628 - accuracy: 0.6297\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 1.1838 - accuracy: 0.5553\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6757 - accuracy: 0.6485\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 1.0911 - accuracy: 0.5814\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6543 - accuracy: 0.6705\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.0399 - accuracy: 0.5481\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7650 - accuracy: 0.5529\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.9246 - accuracy: 0.5834\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6598 - accuracy: 0.6071\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.2401 - accuracy: 0.5525\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7956 - accuracy: 0.5983\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.7616 - accuracy: 0.5887\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6548 - accuracy: 0.6490\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.0064 - accuracy: 0.5204\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7182 - accuracy: 0.5296\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.8600 - accuracy: 0.5756\n",
            "431/431 [==============================] - 1s 3ms/step - loss: 0.6843 - accuracy: 0.6109\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 3.3051 - accuracy: 0.5356\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7567 - accuracy: 0.5237\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 4.3266 - accuracy: 0.5085\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8924 - accuracy: 0.5171\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 0.9246 - accuracy: 0.5185\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6877 - accuracy: 0.5288\n",
            "3871/3871 [==============================] - 10s 2ms/step - loss: 0.7270 - accuracy: 0.5741\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6519 - accuracy: 0.6333\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.9514 - accuracy: 0.5191\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8501 - accuracy: 0.5256\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 4.1864 - accuracy: 0.5156\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 4.1821 - accuracy: 0.5078\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 2.1420 - accuracy: 0.5076\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7386 - accuracy: 0.5149\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.8621 - accuracy: 0.5526\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7024 - accuracy: 0.6482\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.0596 - accuracy: 0.5091\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8111 - accuracy: 0.5144\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 7.7522 - accuracy: 0.4974\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 7.6246 - accuracy: 0.5057\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.9831 - accuracy: 0.5039\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7887 - accuracy: 0.5115\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 1.6437 - accuracy: 0.5192\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7104 - accuracy: 0.5214\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 4.3931 - accuracy: 0.4927\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 4.2808 - accuracy: 0.4984\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 1.4416 - accuracy: 0.5139\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7983 - accuracy: 0.5224\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 0.8026 - accuracy: 0.5100\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7359 - accuracy: 0.5094\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.1001 - accuracy: 0.4938\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8119 - accuracy: 0.5201\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 2.1637 - accuracy: 0.5471\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6548 - accuracy: 0.6383\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.2657 - accuracy: 0.5199\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7533 - accuracy: 0.5220\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.8205 - accuracy: 0.5073\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8087 - accuracy: 0.5222\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.3633 - accuracy: 0.5051\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6713 - accuracy: 0.6394\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.4173 - accuracy: 0.5766\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6699 - accuracy: 0.6102\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 2.6397 - accuracy: 0.6083\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7301 - accuracy: 0.6373\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 1.2840 - accuracy: 0.5607\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6926 - accuracy: 0.6132\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 0.8055 - accuracy: 0.6603\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6938 - accuracy: 0.6607\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.9401 - accuracy: 0.6238\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6672 - accuracy: 0.6102\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.7822 - accuracy: 0.5385\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6915 - accuracy: 0.5897\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.9948 - accuracy: 0.5425\n",
            "431/431 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5659\n",
            "3872/3872 [==============================] - 14s 3ms/step - loss: 2.6300 - accuracy: 0.5233\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7817 - accuracy: 0.5685\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.5662 - accuracy: 0.5628\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7805 - accuracy: 0.6252\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.7197 - accuracy: 0.6485\n",
            "431/431 [==============================] - 1s 3ms/step - loss: 0.5833 - accuracy: 0.7140\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.6482 - accuracy: 0.5932\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6477 - accuracy: 0.6790\n",
            "3871/3871 [==============================] - 11s 3ms/step - loss: 0.7437 - accuracy: 0.5117\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7024 - accuracy: 0.5238\n",
            "3871/3871 [==============================] - 11s 3ms/step - loss: 0.8025 - accuracy: 0.5882\n",
            "431/431 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6820\n",
            "3871/3871 [==============================] - 12s 3ms/step - loss: 0.8958 - accuracy: 0.5986\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6625 - accuracy: 0.6372\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.2065 - accuracy: 0.5591\n",
            "431/431 [==============================] - 1s 3ms/step - loss: 0.7342 - accuracy: 0.6185\n",
            "3872/3872 [==============================] - 14s 3ms/step - loss: 0.8196 - accuracy: 0.6192\n",
            "431/431 [==============================] - 1s 3ms/step - loss: 0.6150 - accuracy: 0.7134\n",
            "3872/3872 [==============================] - 14s 3ms/step - loss: 1.1432 - accuracy: 0.5587\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6959 - accuracy: 0.6015\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 2.5348 - accuracy: 0.5501\n",
            "431/431 [==============================] - 1s 3ms/step - loss: 0.7766 - accuracy: 0.5768\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.1409 - accuracy: 0.6244\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6401 - accuracy: 0.6918\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.8532 - accuracy: 0.5722\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7029 - accuracy: 0.6209\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.7276 - accuracy: 0.5697\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6552 - accuracy: 0.6626\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 0.7145 - accuracy: 0.6150\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6264 - accuracy: 0.6714\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 1.1125 - accuracy: 0.5187\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7190 - accuracy: 0.5235\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 1.9364 - accuracy: 0.5709\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6399\n",
            "3872/3872 [==============================] - 14s 3ms/step - loss: 2.3116 - accuracy: 0.5806\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7118 - accuracy: 0.6532\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.8771 - accuracy: 0.5898\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6866 - accuracy: 0.6165\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.4848 - accuracy: 0.5178\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7571 - accuracy: 0.5466\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.8547 - accuracy: 0.5592\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7613 - accuracy: 0.5789\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.6853 - accuracy: 0.5706\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6270 - accuracy: 0.6805\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.3880 - accuracy: 0.5327\n",
            "431/431 [==============================] - 1s 3ms/step - loss: 0.7646 - accuracy: 0.5878\n",
            "3872/3872 [==============================] - 14s 3ms/step - loss: 1.0597 - accuracy: 0.5334\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6987 - accuracy: 0.6135\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 0.9203 - accuracy: 0.6255\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7793 - accuracy: 0.5908\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 0.7743 - accuracy: 0.6139\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6852 - accuracy: 0.6598\n",
            "3871/3871 [==============================] - 10s 3ms/step - loss: 0.8540 - accuracy: 0.6348\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6528 - accuracy: 0.6586\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.7290 - accuracy: 0.5940\n",
            "431/431 [==============================] - 1s 3ms/step - loss: 0.6463 - accuracy: 0.6351\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.8149 - accuracy: 0.5362\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6432 - accuracy: 0.6533\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 0.8333 - accuracy: 0.6247\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6918 - accuracy: 0.6631\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.0911 - accuracy: 0.5927\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.6590 - accuracy: 0.6365\n",
            "3872/3872 [==============================] - 12s 3ms/step - loss: 1.3829 - accuracy: 0.5375\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7376 - accuracy: 0.5459\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 4.0478 - accuracy: 0.5851\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.8143 - accuracy: 0.6508\n",
            "3872/3872 [==============================] - 13s 3ms/step - loss: 1.2423 - accuracy: 0.5140\n",
            "431/431 [==============================] - 1s 2ms/step - loss: 0.7578 - accuracy: 0.5396\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 4.3135 - accuracy: 0.4941\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 4.2680 - accuracy: 0.4927\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 3.5039 - accuracy: 0.5159\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7014 - accuracy: 0.5162\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 1.1212 - accuracy: 0.5861\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.6927 - accuracy: 0.6405\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 2.7931 - accuracy: 0.5253\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7967 - accuracy: 0.5355\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 7.7375 - accuracy: 0.4984\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 7.7573 - accuracy: 0.4971\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 7.7560 - accuracy: 0.4972\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 7.5905 - accuracy: 0.5079\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 1.6867 - accuracy: 0.5338\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.8246 - accuracy: 0.5308\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 7.7377 - accuracy: 0.4984\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 7.7555 - accuracy: 0.4972\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 7.7522 - accuracy: 0.4974\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 7.6246 - accuracy: 0.5057\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.5229 - accuracy: 0.4996\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.6984 - accuracy: 0.5122\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 0.7943 - accuracy: 0.5383\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.6903 - accuracy: 0.5638\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 5.1259 - accuracy: 0.5002\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 4.0368 - accuracy: 0.5102\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 4.2202 - accuracy: 0.4954\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 4.0886 - accuracy: 0.4984\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 4.3382 - accuracy: 0.5301\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 4.0726 - accuracy: 0.5782\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 3.9195 - accuracy: 0.5351\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.9012 - accuracy: 0.5334\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 1.2893 - accuracy: 0.5259\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7675 - accuracy: 0.5171\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 4.3907 - accuracy: 0.5346\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 4.1847 - accuracy: 0.5987\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 1.2463 - accuracy: 0.5143\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7810 - accuracy: 0.5115\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 2.1873 - accuracy: 0.4999\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7930 - accuracy: 0.5266\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 6.7769 - accuracy: 0.5013\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 4.3377 - accuracy: 0.5093\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 2.3736 - accuracy: 0.5181\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8786 - accuracy: 0.5355\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8949 - accuracy: 0.5270\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7132 - accuracy: 0.5381\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0008 - accuracy: 0.5760\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.6484\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.6438 - accuracy: 0.5510\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7119 - accuracy: 0.5901\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 1.0637 - accuracy: 0.5659\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7503 - accuracy: 0.5445\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.9079 - accuracy: 0.6017\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6806 - accuracy: 0.6304\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.4450 - accuracy: 0.5165\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.9461 - accuracy: 0.5198\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 2.6176 - accuracy: 0.5432\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7581 - accuracy: 0.5957\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 1.2406 - accuracy: 0.5283\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7820 - accuracy: 0.5757\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8778 - accuracy: 0.5058\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7247 - accuracy: 0.5250\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 2.4710 - accuracy: 0.5209\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7106 - accuracy: 0.5499\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 1.9229 - accuracy: 0.5249\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7890 - accuracy: 0.5299\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 1.5064 - accuracy: 0.5207\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7145 - accuracy: 0.5302\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8508 - accuracy: 0.5837\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6726\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.9988 - accuracy: 0.5205\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.6826 - accuracy: 0.5637\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 1.8192 - accuracy: 0.5273\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7499 - accuracy: 0.5528\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 0.6999 - accuracy: 0.5662\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6709\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.1353 - accuracy: 0.5401\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6839 - accuracy: 0.6132\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 1.3217 - accuracy: 0.5159\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7942 - accuracy: 0.5172\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 3.7108 - accuracy: 0.5343\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.8991 - accuracy: 0.5997\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 1.5300 - accuracy: 0.5007\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 1.1510 - accuracy: 0.5196\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.9027 - accuracy: 0.5158\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7608 - accuracy: 0.5578\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.7224 - accuracy: 0.5268\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7629 - accuracy: 0.5274\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8828 - accuracy: 0.5192\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7161 - accuracy: 0.5332\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0621 - accuracy: 0.5634\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7333 - accuracy: 0.5664\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 0.7751 - accuracy: 0.5420\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.6815 - accuracy: 0.6676\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.7466 - accuracy: 0.5738\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6730 - accuracy: 0.6399\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0460 - accuracy: 0.5123\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8081 - accuracy: 0.5234\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.3929 - accuracy: 0.5103\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 3.4175 - accuracy: 0.5280\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.1311 - accuracy: 0.5083\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8149 - accuracy: 0.5112\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0017 - accuracy: 0.6095\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6775 - accuracy: 0.6731\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.3079 - accuracy: 0.5463\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7995 - accuracy: 0.5925\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.5910 - accuracy: 0.5979\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7120 - accuracy: 0.6716\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8577 - accuracy: 0.5240\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7504 - accuracy: 0.5444\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8344 - accuracy: 0.5817\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6768 - accuracy: 0.6605\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 3.0917 - accuracy: 0.5684\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7229 - accuracy: 0.6314\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.2255 - accuracy: 0.5137\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7657 - accuracy: 0.5184\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.3747 - accuracy: 0.5197\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8103 - accuracy: 0.5371\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.1614 - accuracy: 0.6129\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6558\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.1606 - accuracy: 0.5163\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7719 - accuracy: 0.5044\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 2.7677 - accuracy: 0.5147\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8250 - accuracy: 0.5406\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.2629 - accuracy: 0.4967\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 3.9670 - accuracy: 0.5035\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.7618 - accuracy: 0.5860\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6508\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 2.1139 - accuracy: 0.4938\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 1.0198 - accuracy: 0.5050\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.6439 - accuracy: 0.5237\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8456 - accuracy: 0.5410\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.2589 - accuracy: 0.5040\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.8841 - accuracy: 0.4998\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.4911 - accuracy: 0.5266\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7555 - accuracy: 0.5419\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.4939 - accuracy: 0.5030\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 4.3051 - accuracy: 0.5067\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 0.7478 - accuracy: 0.5072\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7083 - accuracy: 0.5065\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 7.7116 - accuracy: 0.5000\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 7.9886 - accuracy: 0.4821\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0174 - accuracy: 0.5969\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7894 - accuracy: 0.6347\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.9775 - accuracy: 0.5205\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7424 - accuracy: 0.5180\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 0.8517 - accuracy: 0.5036\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.6998 - accuracy: 0.5435\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 1.9004 - accuracy: 0.5011\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.9941 - accuracy: 0.4969\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 1.0060 - accuracy: 0.5172\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7285 - accuracy: 0.5130\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.5630 - accuracy: 0.5089\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7688 - accuracy: 0.5227\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.6708 - accuracy: 0.5224\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8665 - accuracy: 0.5062\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.2173 - accuracy: 0.5062\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.9741 - accuracy: 0.5110\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.7990 - accuracy: 0.5178\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6818 - accuracy: 0.5439\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.4583 - accuracy: 0.5192\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7978 - accuracy: 0.5163\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0258 - accuracy: 0.5083\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7881 - accuracy: 0.5196\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.2146 - accuracy: 0.5088\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7099 - accuracy: 0.5166\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.3833 - accuracy: 0.5330\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 4.2843 - accuracy: 0.5242\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0677 - accuracy: 0.5514\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7534 - accuracy: 0.5722\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.4640 - accuracy: 0.5201\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7136 - accuracy: 0.5294\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.6086 - accuracy: 0.5189\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8303 - accuracy: 0.5141\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8699 - accuracy: 0.5266\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7682 - accuracy: 0.5364\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8409 - accuracy: 0.5229\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7089 - accuracy: 0.5242\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 1.0067 - accuracy: 0.5189\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8192 - accuracy: 0.5180\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.2619 - accuracy: 0.5364\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8130 - accuracy: 0.5415\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.9738 - accuracy: 0.6073\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7131 - accuracy: 0.6638\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 0.9568 - accuracy: 0.5171\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8599 - accuracy: 0.5113\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 1.1407 - accuracy: 0.5152\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7679 - accuracy: 0.5384\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.9641 - accuracy: 0.5451\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7075 - accuracy: 0.6007\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8615 - accuracy: 0.4959\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7713 - accuracy: 0.4995\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.7899 - accuracy: 0.5627\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.6903 - accuracy: 0.5868\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.0283 - accuracy: 0.5733\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6955 - accuracy: 0.6361\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.9916 - accuracy: 0.5121\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7730 - accuracy: 0.5264\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.1398 - accuracy: 0.5154\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7083 - accuracy: 0.5117\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.1934 - accuracy: 0.5257\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7172 - accuracy: 0.5629\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.7782 - accuracy: 0.5429\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8010 - accuracy: 0.5900\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8932 - accuracy: 0.5543\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7174 - accuracy: 0.5867\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.7580 - accuracy: 0.6042\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6486\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 3.7869 - accuracy: 0.5199\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7941 - accuracy: 0.5651\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 0.7512 - accuracy: 0.5974\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6068 - accuracy: 0.6832\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 1.8422 - accuracy: 0.5522\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7345 - accuracy: 0.5882\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.5328 - accuracy: 0.5526\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7962 - accuracy: 0.5477\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 2.9986 - accuracy: 0.5406\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7220 - accuracy: 0.5451\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.6843 - accuracy: 0.5384\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7617 - accuracy: 0.5974\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.4168 - accuracy: 0.5248\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7201 - accuracy: 0.5369\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.1299 - accuracy: 0.5280\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7871 - accuracy: 0.5488\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.2142 - accuracy: 0.5212\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7059 - accuracy: 0.5231\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.2073 - accuracy: 0.5481\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.6923 - accuracy: 0.6200\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.6895 - accuracy: 0.6280\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6062 - accuracy: 0.7076\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.7438 - accuracy: 0.5286\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7428 - accuracy: 0.5791\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.3716 - accuracy: 0.5092\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7364 - accuracy: 0.5300\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8123 - accuracy: 0.5753\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7164 - accuracy: 0.5950\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.3156 - accuracy: 0.5564\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7007 - accuracy: 0.6683\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.4783 - accuracy: 0.5158\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8247 - accuracy: 0.5345\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.6437 - accuracy: 0.5771\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6986 - accuracy: 0.6283\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 1.5447 - accuracy: 0.5467\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.9136 - accuracy: 0.5449\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.1472 - accuracy: 0.5148\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 4.0763 - accuracy: 0.5227\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.4597 - accuracy: 0.5207\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8211 - accuracy: 0.5404\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.5349 - accuracy: 0.4949\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 4.3524 - accuracy: 0.4904\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.2944 - accuracy: 0.5021\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.9688 - accuracy: 0.5067\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.1581 - accuracy: 0.5141\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7561 - accuracy: 0.5098\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.9791 - accuracy: 0.5226\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8781 - accuracy: 0.5215\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.1982 - accuracy: 0.5081\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 4.0555 - accuracy: 0.5341\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8496 - accuracy: 0.5135\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7685 - accuracy: 0.5195\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.7503 - accuracy: 0.5103\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7081 - accuracy: 0.4830\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.6768 - accuracy: 0.5145\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7564 - accuracy: 0.5298\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 5.0541 - accuracy: 0.4983\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 4.1802 - accuracy: 0.4980\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.7386 - accuracy: 0.5046\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6841 - accuracy: 0.5832\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.9952 - accuracy: 0.5104\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7561 - accuracy: 0.5170\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 4.1581 - accuracy: 0.5883\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 4.1698 - accuracy: 0.5990\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.9526 - accuracy: 0.5149\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7320 - accuracy: 0.5322\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0887 - accuracy: 0.5618\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6842 - accuracy: 0.5986\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.1369 - accuracy: 0.4976\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7125 - accuracy: 0.5099\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.1607 - accuracy: 0.4973\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8704 - accuracy: 0.5053\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.1791 - accuracy: 0.5128\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8327 - accuracy: 0.5115\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.3347 - accuracy: 0.5199\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7700 - accuracy: 0.5342\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 3.0828 - accuracy: 0.5083\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7112 - accuracy: 0.5506\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.3121 - accuracy: 0.5615\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7410 - accuracy: 0.6486\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8971 - accuracy: 0.4958\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8175 - accuracy: 0.5045\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.3884 - accuracy: 0.5490\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6714 - accuracy: 0.6065\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.4936 - accuracy: 0.5197\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7401 - accuracy: 0.5441\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8555 - accuracy: 0.5319\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7394 - accuracy: 0.5287\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.7514 - accuracy: 0.5200\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7068 - accuracy: 0.5214\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.2016 - accuracy: 0.5502\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7319 - accuracy: 0.6063\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 0.9241 - accuracy: 0.5121\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8163 - accuracy: 0.5148\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.4790 - accuracy: 0.5067\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.9122 - accuracy: 0.5207\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.7115 - accuracy: 0.6175\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6399 - accuracy: 0.6635\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.2331 - accuracy: 0.5190\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8875 - accuracy: 0.5071\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.9873 - accuracy: 0.5636\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7631 - accuracy: 0.6561\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0846 - accuracy: 0.5346\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7890 - accuracy: 0.5328\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0967 - accuracy: 0.5425\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7392 - accuracy: 0.5873\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8177 - accuracy: 0.5102\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7294 - accuracy: 0.5179\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.6313 - accuracy: 0.5396\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7069 - accuracy: 0.5928\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.0407 - accuracy: 0.5511\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7510 - accuracy: 0.5861\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.3306 - accuracy: 0.5064\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 4.2391 - accuracy: 0.5263\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.9395 - accuracy: 0.5129\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7502 - accuracy: 0.5295\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.4070 - accuracy: 0.5272\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7812 - accuracy: 0.5668\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 3.1629 - accuracy: 0.5249\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8262 - accuracy: 0.5436\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.0837 - accuracy: 0.5389\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8317 - accuracy: 0.5535\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 4.4296 - accuracy: 0.5048\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 4.2924 - accuracy: 0.4995\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.7818 - accuracy: 0.5225\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8237 - accuracy: 0.5407\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.3938 - accuracy: 0.6078\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7201 - accuracy: 0.6450\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8712 - accuracy: 0.5797\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7463 - accuracy: 0.6480\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.9618 - accuracy: 0.5198\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7434 - accuracy: 0.5396\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.9457 - accuracy: 0.5434\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6872 - accuracy: 0.5681\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 2.0917 - accuracy: 0.5749\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7310 - accuracy: 0.6282\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.2713 - accuracy: 0.5669\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7729 - accuracy: 0.6418\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 3.1055 - accuracy: 0.5230\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.9032 - accuracy: 0.5411\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.1276 - accuracy: 0.5237\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8599 - accuracy: 0.5343\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.1958 - accuracy: 0.5170\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7488 - accuracy: 0.5188\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.7679 - accuracy: 0.5720\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7053 - accuracy: 0.6694\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.6073 - accuracy: 0.5330\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7239 - accuracy: 0.5292\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.5966 - accuracy: 0.5841\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7331 - accuracy: 0.6533\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0886 - accuracy: 0.5172\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7827 - accuracy: 0.5264\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.9111 - accuracy: 0.6167\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7084 - accuracy: 0.6769\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.1267 - accuracy: 0.5487\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8640 - accuracy: 0.5377\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8158 - accuracy: 0.5152\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7774 - accuracy: 0.5124\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0767 - accuracy: 0.5705\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7140 - accuracy: 0.5817\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.6422 - accuracy: 0.5016\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8434 - accuracy: 0.5021\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.9559 - accuracy: 0.5024\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7661 - accuracy: 0.4991\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.8500 - accuracy: 0.5017\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 4.2236 - accuracy: 0.5080\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8774 - accuracy: 0.5958\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7643 - accuracy: 0.6711\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0363 - accuracy: 0.4992\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8082 - accuracy: 0.5008\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.2962 - accuracy: 0.5018\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 4.1201 - accuracy: 0.5002\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 1.0687 - accuracy: 0.5155\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.8530 - accuracy: 0.5126\n",
            "1549/1549 [==============================] - 5s 3ms/step - loss: 1.7650 - accuracy: 0.5568\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7987 - accuracy: 0.5788\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.4505 - accuracy: 0.5028\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 4.1554 - accuracy: 0.5210\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0312 - accuracy: 0.5197\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8092 - accuracy: 0.5247\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8976 - accuracy: 0.5164\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.8184 - accuracy: 0.5233\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8340 - accuracy: 0.5039\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7394 - accuracy: 0.5072\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.8110 - accuracy: 0.5082\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7872 - accuracy: 0.5137\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 4.0006 - accuracy: 0.5570\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.9551 - accuracy: 0.5181\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 3.3239 - accuracy: 0.5231\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7327 - accuracy: 0.5267\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.2717 - accuracy: 0.5868\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 4.0550 - accuracy: 0.6187\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.4380 - accuracy: 0.5647\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6990 - accuracy: 0.6016\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.5240 - accuracy: 0.5267\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.8696 - accuracy: 0.5412\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.7786 - accuracy: 0.5386\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7004 - accuracy: 0.6136\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 1.2601 - accuracy: 0.5147\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7686 - accuracy: 0.5171\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.2057 - accuracy: 0.5097\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 4.1351 - accuracy: 0.5396\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0948 - accuracy: 0.5717\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7558 - accuracy: 0.6238\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.5154 - accuracy: 0.5176\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7264 - accuracy: 0.5338\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 3.1756 - accuracy: 0.5045\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8234 - accuracy: 0.5087\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.3822 - accuracy: 0.5267\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7219 - accuracy: 0.5366\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.6133 - accuracy: 0.5319\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7780 - accuracy: 0.5352\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.9147 - accuracy: 0.5120\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7918 - accuracy: 0.5379\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.5738 - accuracy: 0.5474\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7890 - accuracy: 0.5641\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 3.5475 - accuracy: 0.5431\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.9056 - accuracy: 0.6120\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 2.7920 - accuracy: 0.5092\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7913 - accuracy: 0.5405\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.2619 - accuracy: 0.5395\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 3.8897 - accuracy: 0.5487\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.6894 - accuracy: 0.5230\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7864 - accuracy: 0.5286\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8397 - accuracy: 0.5434\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6672\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0087 - accuracy: 0.5017\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8192 - accuracy: 0.5153\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.6284 - accuracy: 0.5414\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7158 - accuracy: 0.5723\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.2608 - accuracy: 0.5124\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8450 - accuracy: 0.5209\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.3644 - accuracy: 0.5095\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 1.1514 - accuracy: 0.5067\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.5055 - accuracy: 0.5559\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.8397 - accuracy: 0.6325\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 3.5552 - accuracy: 0.5066\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8985 - accuracy: 0.5448\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.3902 - accuracy: 0.5586\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7448 - accuracy: 0.5478\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.2242 - accuracy: 0.5227\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7378 - accuracy: 0.5744\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.2008 - accuracy: 0.5650\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 4.2072 - accuracy: 0.5841\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.5396 - accuracy: 0.5178\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8149 - accuracy: 0.5516\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 4.3661 - accuracy: 0.5720\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 4.2642 - accuracy: 0.5979\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8520 - accuracy: 0.5246\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7639 - accuracy: 0.5572\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0399 - accuracy: 0.5427\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7074 - accuracy: 0.5772\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 1.8949 - accuracy: 0.5295\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7155 - accuracy: 0.5452\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.8466 - accuracy: 0.5186\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7354 - accuracy: 0.5296\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 0.8400 - accuracy: 0.5126\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7217 - accuracy: 0.5318\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.9553 - accuracy: 0.5235\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7925 - accuracy: 0.5250\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 2.3000 - accuracy: 0.5184\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7248 - accuracy: 0.5165\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0494 - accuracy: 0.6058\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6730\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0574 - accuracy: 0.5236\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.8188 - accuracy: 0.5322\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.9533 - accuracy: 0.5622\n",
            "173/173 [==============================] - 1s 2ms/step - loss: 0.7078 - accuracy: 0.6650\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 0.7966 - accuracy: 0.5145\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7367 - accuracy: 0.5138\n",
            "1549/1549 [==============================] - 6s 4ms/step - loss: 1.4516 - accuracy: 0.5182\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7579 - accuracy: 0.5216\n",
            "1549/1549 [==============================] - 6s 3ms/step - loss: 1.0116 - accuracy: 0.5115\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.7900 - accuracy: 0.5148\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 2.2041 - accuracy: 0.5113\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7071 - accuracy: 0.5158\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 2.1917 - accuracy: 0.5442\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7102 - accuracy: 0.6262\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7688 - accuracy: 0.5637\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6399 - accuracy: 0.6526\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.3878 - accuracy: 0.5000\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7937 - accuracy: 0.5108\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 4.0691 - accuracy: 0.5847\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 4.0912 - accuracy: 0.6089\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.1491 - accuracy: 0.5390\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7872 - accuracy: 0.5427\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.6388 - accuracy: 0.5281\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7774 - accuracy: 0.5594\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 4.0270 - accuracy: 0.5672\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 1.0036 - accuracy: 0.5320\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7959 - accuracy: 0.5144\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6963 - accuracy: 0.5339\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 3.3397 - accuracy: 0.5677\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7643 - accuracy: 0.6239\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0189 - accuracy: 0.5214\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6947 - accuracy: 0.5552\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9564 - accuracy: 0.5077\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7073 - accuracy: 0.5075\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 4.3895 - accuracy: 0.4917\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 4.1679 - accuracy: 0.4973\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0824 - accuracy: 0.5194\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6870 - accuracy: 0.5239\n",
            "3097/3097 [==============================] - 12s 3ms/step - loss: 1.2191 - accuracy: 0.5672\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6906 - accuracy: 0.6523\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 3.7228 - accuracy: 0.5299\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7934 - accuracy: 0.5439\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 3.2531 - accuracy: 0.5305\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.9693 - accuracy: 0.5378\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 1.7097 - accuracy: 0.5157\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7254 - accuracy: 0.5242\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 3.0202 - accuracy: 0.5574\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7477 - accuracy: 0.6237\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 0.7203 - accuracy: 0.6295\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6182 - accuracy: 0.6814\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7965 - accuracy: 0.5936\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6545\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.1409 - accuracy: 0.5646\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.6127\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.2374 - accuracy: 0.5375\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.8163 - accuracy: 0.5348\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9151 - accuracy: 0.5168\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7514 - accuracy: 0.5157\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.6423 - accuracy: 0.5106\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7070 - accuracy: 0.5164\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.7760 - accuracy: 0.5972\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6991\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8871 - accuracy: 0.5839\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.6631\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 3.0500 - accuracy: 0.5347\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.8831 - accuracy: 0.5720\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0363 - accuracy: 0.5878\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6733\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7736 - accuracy: 0.5295\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6856 - accuracy: 0.5503\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0571 - accuracy: 0.5303\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7172 - accuracy: 0.6014\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9438 - accuracy: 0.5437\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7887 - accuracy: 0.5778\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 1.3415 - accuracy: 0.5094\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7554 - accuracy: 0.5171\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 0.8393 - accuracy: 0.5390\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5877\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8186 - accuracy: 0.6067\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6846 - accuracy: 0.6472\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8994 - accuracy: 0.5064\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7191 - accuracy: 0.5199\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 0.7540 - accuracy: 0.5829\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6776 - accuracy: 0.6045\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 1.3029 - accuracy: 0.5133\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7758 - accuracy: 0.5180\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 1.0065 - accuracy: 0.5415\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6737 - accuracy: 0.6293\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 3.0318 - accuracy: 0.5101\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.9445 - accuracy: 0.5231\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 1.6649 - accuracy: 0.5306\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.8068 - accuracy: 0.4985\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 0.8352 - accuracy: 0.5217\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7445 - accuracy: 0.5214\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 1.5137 - accuracy: 0.5731\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6813\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 0.7793 - accuracy: 0.5439\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7192 - accuracy: 0.5571\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 1.1202 - accuracy: 0.5884\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6879 - accuracy: 0.6782\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 0.8355 - accuracy: 0.6079\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6908\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8943 - accuracy: 0.5717\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6490 - accuracy: 0.6690\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7974 - accuracy: 0.5883\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6604\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9140 - accuracy: 0.5998\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.5966 - accuracy: 0.7136\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9556 - accuracy: 0.5762\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6850 - accuracy: 0.6500\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8125 - accuracy: 0.5940\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6971\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.6255 - accuracy: 0.5181\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7272 - accuracy: 0.5202\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 1.4836 - accuracy: 0.5662\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7132 - accuracy: 0.6132\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 0.9057 - accuracy: 0.5226\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7927 - accuracy: 0.5030\n",
            "3097/3097 [==============================] - 13s 4ms/step - loss: 0.9318 - accuracy: 0.5862\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6753\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 3.7954 - accuracy: 0.5583\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7616 - accuracy: 0.6260\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 1.6742 - accuracy: 0.5482\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6972 - accuracy: 0.6221\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 0.9719 - accuracy: 0.5164\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7017 - accuracy: 0.5424\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 2.0721 - accuracy: 0.5088\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7936 - accuracy: 0.5155\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 1.3447 - accuracy: 0.5997\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.5932\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 4.2808 - accuracy: 0.5204\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 3.9027 - accuracy: 0.5490\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 0.8147 - accuracy: 0.5105\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7565 - accuracy: 0.5025\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 2.1629 - accuracy: 0.5023\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7536 - accuracy: 0.5149\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 4.3328 - accuracy: 0.5605\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 1.0991 - accuracy: 0.5241\n",
            "3097/3097 [==============================] - 12s 3ms/step - loss: 2.3272 - accuracy: 0.5003\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7727 - accuracy: 0.5119\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 0.8448 - accuracy: 0.5811\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6864\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 1.4267 - accuracy: 0.5867\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6492\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.2282 - accuracy: 0.5180\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7255 - accuracy: 0.5207\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 0.8594 - accuracy: 0.5096\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7720 - accuracy: 0.5253\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.7154 - accuracy: 0.5219\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.7743 - accuracy: 0.5326\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7211 - accuracy: 0.5218\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6635 - accuracy: 0.5930\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 2.8803 - accuracy: 0.5047\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7295 - accuracy: 0.5184\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.3313 - accuracy: 0.4975\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7278 - accuracy: 0.5044\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9820 - accuracy: 0.5043\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7170 - accuracy: 0.5200\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.6783 - accuracy: 0.5201\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7057 - accuracy: 0.5262\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0098 - accuracy: 0.5040\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7440 - accuracy: 0.5165\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 1.7740 - accuracy: 0.5066\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7420 - accuracy: 0.5369\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 3.9192 - accuracy: 0.5027\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.8013 - accuracy: 0.5126\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.1432 - accuracy: 0.5411\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6721 - accuracy: 0.6006\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9888 - accuracy: 0.5116\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7469 - accuracy: 0.4819\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 3.5519 - accuracy: 0.5202\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7318 - accuracy: 0.5842\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.2018 - accuracy: 0.5188\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.7362 - accuracy: 0.5202\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.1345 - accuracy: 0.5846\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.6941 - accuracy: 0.6456\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.2995 - accuracy: 0.5494\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6552 - accuracy: 0.6522\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9793 - accuracy: 0.5636\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7645 - accuracy: 0.6072\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 7.7556 - accuracy: 0.4972\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 7.5892 - accuracy: 0.5079\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9205 - accuracy: 0.5169\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.6868 - accuracy: 0.5909\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8401 - accuracy: 0.5799\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6868 - accuracy: 0.6548\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 2.6730 - accuracy: 0.5180\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7973 - accuracy: 0.4866\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8623 - accuracy: 0.5893\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6557 - accuracy: 0.6372\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0017 - accuracy: 0.5521\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7018 - accuracy: 0.5935\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.7933 - accuracy: 0.5271\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7550 - accuracy: 0.5248\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8546 - accuracy: 0.5095\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7332 - accuracy: 0.5231\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 2.2066 - accuracy: 0.6087\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6422\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.5265 - accuracy: 0.5665\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7424 - accuracy: 0.6169\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 2.1939 - accuracy: 0.5305\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.8305 - accuracy: 0.5498\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9869 - accuracy: 0.5195\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7276 - accuracy: 0.5212\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8520 - accuracy: 0.5135\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7141 - accuracy: 0.5038\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.4787 - accuracy: 0.5135\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.7083 - accuracy: 0.5305\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0654 - accuracy: 0.5719\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6736 - accuracy: 0.6169\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0896 - accuracy: 0.5631\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.6518 - accuracy: 0.6470\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.2517 - accuracy: 0.5423\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7101 - accuracy: 0.5887\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.3975 - accuracy: 0.5515\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7755 - accuracy: 0.5253\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8730 - accuracy: 0.5452\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6717 - accuracy: 0.6115\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8339 - accuracy: 0.5480\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.6628 - accuracy: 0.6280\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 2.7397 - accuracy: 0.5983\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6569\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.1061 - accuracy: 0.5312\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7284 - accuracy: 0.5425\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 1.3880 - accuracy: 0.5733\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.7252 - accuracy: 0.6610\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.3619 - accuracy: 0.5093\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.8040 - accuracy: 0.5067\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8729 - accuracy: 0.5305\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7003 - accuracy: 0.5515\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8941 - accuracy: 0.5791\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7094 - accuracy: 0.6673\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9497 - accuracy: 0.5170\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7535 - accuracy: 0.5268\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 3.5464 - accuracy: 0.5273\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7422 - accuracy: 0.5416\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9002 - accuracy: 0.5511\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7601 - accuracy: 0.5765\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8951 - accuracy: 0.5590\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.7574 - accuracy: 0.5658\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 1.8318 - accuracy: 0.5935\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.6390\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.1227 - accuracy: 0.5685\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7443 - accuracy: 0.6236\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7607 - accuracy: 0.5978\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6713 - accuracy: 0.6319\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0352 - accuracy: 0.5632\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7701 - accuracy: 0.6029\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0995 - accuracy: 0.5304\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7201 - accuracy: 0.5408\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 4.5952 - accuracy: 0.4936\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 4.3834 - accuracy: 0.4985\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 2.3364 - accuracy: 0.5542\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.9137 - accuracy: 0.5714\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 4.1456 - accuracy: 0.5389\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 3.9988 - accuracy: 0.5585\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 4.3837 - accuracy: 0.5398\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 4.2743 - accuracy: 0.5900\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.6938 - accuracy: 0.5938\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6593\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 4.2358 - accuracy: 0.5450\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 3.9274 - accuracy: 0.6157\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7611 - accuracy: 0.5630\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6814 - accuracy: 0.6616\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 4.3801 - accuracy: 0.4973\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 4.2865 - accuracy: 0.5023\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 7.3974 - accuracy: 0.4983\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 3.5970 - accuracy: 0.5179\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 2.4252 - accuracy: 0.5266\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.8504 - accuracy: 0.5155\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 3.4204 - accuracy: 0.5039\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.8356 - accuracy: 0.4922\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 4.4150 - accuracy: 0.4998\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 4.1844 - accuracy: 0.5011\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.2686 - accuracy: 0.5288\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7461 - accuracy: 0.5360\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 4.1532 - accuracy: 0.5742\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 3.9209 - accuracy: 0.6158\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0700 - accuracy: 0.5124\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7455 - accuracy: 0.5114\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8060 - accuracy: 0.5350\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6837 - accuracy: 0.6304\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9201 - accuracy: 0.5055\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7192 - accuracy: 0.5286\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 4.4558 - accuracy: 0.4930\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 4.2480 - accuracy: 0.5348\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8633 - accuracy: 0.5302\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7792 - accuracy: 0.5556\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7637 - accuracy: 0.5917\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6496 - accuracy: 0.6555\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7539 - accuracy: 0.5622\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.6106\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 4.5900 - accuracy: 0.5003\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 4.2117 - accuracy: 0.5061\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 3.6586 - accuracy: 0.5418\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.8096 - accuracy: 0.5440\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 1.4734 - accuracy: 0.5465\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7112 - accuracy: 0.6378\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7467 - accuracy: 0.5399\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6092\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 0.9877 - accuracy: 0.6223\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.5934 - accuracy: 0.7001\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.4795 - accuracy: 0.5755\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.6628\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.9776 - accuracy: 0.6013\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6113 - accuracy: 0.6917\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8249 - accuracy: 0.5142\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7328 - accuracy: 0.5158\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 2.5909 - accuracy: 0.5211\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7665 - accuracy: 0.5246\n",
            "3097/3097 [==============================] - 13s 4ms/step - loss: 0.7268 - accuracy: 0.6358\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6014 - accuracy: 0.7036\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7382 - accuracy: 0.6131\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6720 - accuracy: 0.6484\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8843 - accuracy: 0.5782\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6035 - accuracy: 0.6994\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 1.0237 - accuracy: 0.5314\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7378 - accuracy: 0.5306\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9474 - accuracy: 0.5181\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7203 - accuracy: 0.5452\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0698 - accuracy: 0.5131\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7627 - accuracy: 0.5257\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0113 - accuracy: 0.5495\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7476 - accuracy: 0.5795\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.4583 - accuracy: 0.5186\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5312\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 1.0961 - accuracy: 0.5178\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7637 - accuracy: 0.5200\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7120 - accuracy: 0.6057\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.6583\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 2.7818 - accuracy: 0.5733\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.9133 - accuracy: 0.5994\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.5735 - accuracy: 0.5602\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6997 - accuracy: 0.6147\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.6253 - accuracy: 0.6766\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.5493 - accuracy: 0.7366\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9178 - accuracy: 0.5139\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7463 - accuracy: 0.5233\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.1500 - accuracy: 0.5120\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7180 - accuracy: 0.5086\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 1.1892 - accuracy: 0.5252\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7837 - accuracy: 0.5520\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 1.0412 - accuracy: 0.5294\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7128 - accuracy: 0.5396\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.6887 - accuracy: 0.6373\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6940\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.1212 - accuracy: 0.5461\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7268 - accuracy: 0.5906\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.1812 - accuracy: 0.5700\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.8068 - accuracy: 0.5321\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7203 - accuracy: 0.6247\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6038 - accuracy: 0.7192\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9498 - accuracy: 0.5479\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5918\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 3.2771 - accuracy: 0.5157\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.9028 - accuracy: 0.5303\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 0.7322 - accuracy: 0.5927\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6443 - accuracy: 0.6052\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0326 - accuracy: 0.5565\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7395 - accuracy: 0.5580\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8265 - accuracy: 0.5231\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7309 - accuracy: 0.5282\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 3.1319 - accuracy: 0.5463\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7303 - accuracy: 0.6372\n",
            "3097/3097 [==============================] - 12s 3ms/step - loss: 1.5606 - accuracy: 0.5445\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7065 - accuracy: 0.5523\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.4481 - accuracy: 0.5293\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6738 - accuracy: 0.6137\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.2850 - accuracy: 0.5874\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6750\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 4.3869 - accuracy: 0.5104\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 4.2462 - accuracy: 0.5074\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 3.8917 - accuracy: 0.5035\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7813 - accuracy: 0.5170\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 1.0770 - accuracy: 0.5075\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7683 - accuracy: 0.5153\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7530 - accuracy: 0.6083\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6697\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 1.9661 - accuracy: 0.5094\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6957 - accuracy: 0.5157\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 1.2131 - accuracy: 0.5131\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7311 - accuracy: 0.5470\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 6.4801 - accuracy: 0.4995\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.9031 - accuracy: 0.5117\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 3.0827 - accuracy: 0.5358\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.8262 - accuracy: 0.5344\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 2.1626 - accuracy: 0.5522\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.8955 - accuracy: 0.5505\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 3.5264 - accuracy: 0.5038\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.9522 - accuracy: 0.4822\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 1.2198 - accuracy: 0.5185\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7417 - accuracy: 0.5282\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 2.3967 - accuracy: 0.5263\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7732 - accuracy: 0.5470\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 3.7854 - accuracy: 0.5175\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.9226 - accuracy: 0.5110\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.3781 - accuracy: 0.5830\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6818 - accuracy: 0.6554\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.5622 - accuracy: 0.5263\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 1.3642 - accuracy: 0.5587\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 0.8547 - accuracy: 0.5038\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7156 - accuracy: 0.5029\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.5232 - accuracy: 0.5171\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.8819 - accuracy: 0.5233\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0393 - accuracy: 0.5078\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7732 - accuracy: 0.5005\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 0.9356 - accuracy: 0.5098\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7680 - accuracy: 0.5291\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 1.4278 - accuracy: 0.5306\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.8325 - accuracy: 0.5348\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0184 - accuracy: 0.5252\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.6116\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 2.8977 - accuracy: 0.5202\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7537 - accuracy: 0.5241\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0134 - accuracy: 0.6257\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6139 - accuracy: 0.6999\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.5181 - accuracy: 0.6250\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6602\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7560 - accuracy: 0.5868\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6955\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.1387 - accuracy: 0.6202\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6785 - accuracy: 0.6768\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.1839 - accuracy: 0.5721\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7052 - accuracy: 0.6139\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7439 - accuracy: 0.6200\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6562 - accuracy: 0.6708\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0792 - accuracy: 0.5927\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7349 - accuracy: 0.6492\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8773 - accuracy: 0.5834\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6410\n",
            "3097/3097 [==============================] - 13s 4ms/step - loss: 1.2402 - accuracy: 0.5171\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7826 - accuracy: 0.5405\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.7227 - accuracy: 0.6017\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.6218\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.1166 - accuracy: 0.5184\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7544 - accuracy: 0.5430\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.6773 - accuracy: 0.5315\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7625 - accuracy: 0.5363\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.6816 - accuracy: 0.5250\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6970 - accuracy: 0.5739\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 1.1975 - accuracy: 0.5619\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6632\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.8626 - accuracy: 0.5592\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6741 - accuracy: 0.6330\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0155 - accuracy: 0.5195\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7248 - accuracy: 0.5272\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.7053 - accuracy: 0.5992\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6757 - accuracy: 0.6443\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.6993 - accuracy: 0.6034\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6752\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8292 - accuracy: 0.5492\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7333 - accuracy: 0.5889\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.3161 - accuracy: 0.5173\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6959 - accuracy: 0.5875\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.4132 - accuracy: 0.5098\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7190 - accuracy: 0.5211\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.8786 - accuracy: 0.5262\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.6120\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.0005 - accuracy: 0.5976\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7252 - accuracy: 0.6367\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9940 - accuracy: 0.5103\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7706 - accuracy: 0.5252\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 3.9191 - accuracy: 0.5651\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7919 - accuracy: 0.6218\n",
            "3097/3097 [==============================] - 12s 3ms/step - loss: 1.1439 - accuracy: 0.5218\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.8120 - accuracy: 0.5136\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.8606 - accuracy: 0.5772\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.6299\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9102 - accuracy: 0.5877\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7155 - accuracy: 0.6297\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7937 - accuracy: 0.5924\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6824\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.2810 - accuracy: 0.5633\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7178 - accuracy: 0.5882\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 1.5321 - accuracy: 0.5337\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.8039 - accuracy: 0.5497\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9747 - accuracy: 0.5813\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7435 - accuracy: 0.6306\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.7256 - accuracy: 0.5616\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6430\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 1.2174 - accuracy: 0.5498\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7109 - accuracy: 0.6150\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 1.0403 - accuracy: 0.6193\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6506 - accuracy: 0.6936\n",
            "3097/3097 [==============================] - 11s 3ms/step - loss: 0.9399 - accuracy: 0.5202\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7560 - accuracy: 0.5557\n",
            "3097/3097 [==============================] - 12s 4ms/step - loss: 0.9509 - accuracy: 0.5886\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.7179 - accuracy: 0.5960\n",
            "3097/3097 [==============================] - 11s 4ms/step - loss: 0.7900 - accuracy: 0.5967\n",
            "345/345 [==============================] - 1s 3ms/step - loss: 0.6682 - accuracy: 0.6472\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 0.9425 - accuracy: 0.5195\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.5655\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 1.9396 - accuracy: 0.5143\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7318 - accuracy: 0.5065\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 2.2812 - accuracy: 0.5173\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7892 - accuracy: 0.5277\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 3.1183 - accuracy: 0.5264\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7164 - accuracy: 0.5252\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.1053 - accuracy: 0.5057\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8011 - accuracy: 0.5264\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 1.0877 - accuracy: 0.5301\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8538 - accuracy: 0.5307\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 1.4236 - accuracy: 0.5127\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7658 - accuracy: 0.5095\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 0.9297 - accuracy: 0.5167\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5209\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 4.4840 - accuracy: 0.4995\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 4.2708 - accuracy: 0.4972\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 2.0891 - accuracy: 0.4956\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7322 - accuracy: 0.5119\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.3100 - accuracy: 0.5261\n",
            "269/269 [==============================] - 1s 4ms/step - loss: 0.7513 - accuracy: 0.5567\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.2804 - accuracy: 0.5103\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.9559 - accuracy: 0.5027\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 0.7949 - accuracy: 0.5100\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7446 - accuracy: 0.5138\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 0.7505 - accuracy: 0.5154\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6888 - accuracy: 0.5023\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.5950 - accuracy: 0.5040\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8098 - accuracy: 0.5034\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 1.4685 - accuracy: 0.5153\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5379\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 1.4774 - accuracy: 0.5179\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7922 - accuracy: 0.5058\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7318 - accuracy: 0.5246\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6818 - accuracy: 0.5818\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 7.7522 - accuracy: 0.4974\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 7.6246 - accuracy: 0.5057\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0501 - accuracy: 0.5104\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8085 - accuracy: 0.5180\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 0.7216 - accuracy: 0.5761\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6616\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.1502 - accuracy: 0.5267\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8283 - accuracy: 0.5399\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 2.6181 - accuracy: 0.5181\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7376 - accuracy: 0.5321\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9099 - accuracy: 0.5052\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8186 - accuracy: 0.5194\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7855 - accuracy: 0.6029\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7112 - accuracy: 0.6596\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.8976 - accuracy: 0.5280\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7440 - accuracy: 0.5572\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 1.2820 - accuracy: 0.5546\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7267 - accuracy: 0.6275\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.5532 - accuracy: 0.5191\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8402 - accuracy: 0.5199\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.8254 - accuracy: 0.5252\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7301 - accuracy: 0.6060\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 4.4415 - accuracy: 0.5429\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.9075 - accuracy: 0.5573\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9145 - accuracy: 0.5700\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6763\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 0.8465 - accuracy: 0.5733\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6665 - accuracy: 0.6362\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 0.9224 - accuracy: 0.5159\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7497 - accuracy: 0.5290\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9506 - accuracy: 0.5517\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7797 - accuracy: 0.5875\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 3.1294 - accuracy: 0.5495\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7479 - accuracy: 0.5781\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.4017 - accuracy: 0.6062\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6662\n",
            "2420/2420 [==============================] - 10s 4ms/step - loss: 1.1956 - accuracy: 0.5268\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.9199 - accuracy: 0.5343\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 3.6522 - accuracy: 0.5404\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.9187 - accuracy: 0.5478\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 3.5893 - accuracy: 0.5447\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7756 - accuracy: 0.6409\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 3.4079 - accuracy: 0.5870\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7334 - accuracy: 0.6269\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 1.2066 - accuracy: 0.5510\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.6179\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 5.5248 - accuracy: 0.5155\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.9170 - accuracy: 0.5491\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0976 - accuracy: 0.5630\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6781 - accuracy: 0.6165\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.7406 - accuracy: 0.5121\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7007 - accuracy: 0.5165\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 1.1440 - accuracy: 0.5416\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7991 - accuracy: 0.6035\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.3450 - accuracy: 0.5378\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7326 - accuracy: 0.5391\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 2.6895 - accuracy: 0.6173\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6810\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 1.1114 - accuracy: 0.5053\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8923 - accuracy: 0.5186\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 1.2949 - accuracy: 0.5126\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7071 - accuracy: 0.5181\n",
            "2420/2420 [==============================] - 10s 4ms/step - loss: 1.1578 - accuracy: 0.5227\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8388 - accuracy: 0.5395\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 0.7688 - accuracy: 0.6009\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6128 - accuracy: 0.6778\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 1.6159 - accuracy: 0.5377\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8066 - accuracy: 0.5224\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.8541 - accuracy: 0.5593\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5876\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.8846 - accuracy: 0.5505\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6891 - accuracy: 0.6250\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 1.0633 - accuracy: 0.6020\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.7039\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.6954 - accuracy: 0.5100\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.9215 - accuracy: 0.5069\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.4972 - accuracy: 0.5880\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7135 - accuracy: 0.6163\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.8222 - accuracy: 0.5287\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7267 - accuracy: 0.6045\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.8015 - accuracy: 0.5431\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6684 - accuracy: 0.6264\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0208 - accuracy: 0.5464\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7828 - accuracy: 0.5518\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.2694 - accuracy: 0.5123\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7051 - accuracy: 0.5082\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.8570 - accuracy: 0.5155\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6952 - accuracy: 0.5170\n",
            "2420/2420 [==============================] - 10s 4ms/step - loss: 4.1539 - accuracy: 0.5614\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 4.0631 - accuracy: 0.6013\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.6716 - accuracy: 0.4976\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7813 - accuracy: 0.5250\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9165 - accuracy: 0.5232\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8008 - accuracy: 0.5136\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 3.8219 - accuracy: 0.5278\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7945 - accuracy: 0.5792\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.2862 - accuracy: 0.5214\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7755 - accuracy: 0.5993\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.4339 - accuracy: 0.5150\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6972 - accuracy: 0.5432\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.2400 - accuracy: 0.5164\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8251 - accuracy: 0.5081\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0433 - accuracy: 0.5627\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6455 - accuracy: 0.6200\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 1.2499 - accuracy: 0.5072\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5440\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.6425 - accuracy: 0.6652\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6114 - accuracy: 0.6959\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 3.0321 - accuracy: 0.5071\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7670 - accuracy: 0.4982\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 1.0822 - accuracy: 0.5147\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6970 - accuracy: 0.5072\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7533 - accuracy: 0.5315\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6748 - accuracy: 0.5637\n",
            "2420/2420 [==============================] - 11s 4ms/step - loss: 1.3460 - accuracy: 0.5797\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6496 - accuracy: 0.6244\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 5.9380 - accuracy: 0.5009\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 4.0883 - accuracy: 0.5081\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.3251 - accuracy: 0.5294\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7322 - accuracy: 0.5313\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.8645 - accuracy: 0.5084\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7905 - accuracy: 0.5178\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.3176 - accuracy: 0.4992\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 4.3047 - accuracy: 0.4802\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.8368 - accuracy: 0.5741\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6431\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7467 - accuracy: 0.5802\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6092\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.8734 - accuracy: 0.5631\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.6620\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9751 - accuracy: 0.5330\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7786 - accuracy: 0.5365\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9375 - accuracy: 0.5552\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7726 - accuracy: 0.5606\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9143 - accuracy: 0.5118\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7502 - accuracy: 0.5287\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9435 - accuracy: 0.5215\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8032 - accuracy: 0.5308\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.1739 - accuracy: 0.5003\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8530 - accuracy: 0.5146\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9844 - accuracy: 0.5631\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6835 - accuracy: 0.6219\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0288 - accuracy: 0.5231\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7762 - accuracy: 0.4914\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 5.7808 - accuracy: 0.4955\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 4.2590 - accuracy: 0.5132\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.1171 - accuracy: 0.5865\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6524\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.5790 - accuracy: 0.5114\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8841 - accuracy: 0.5417\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 3.4929 - accuracy: 0.5298\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8242 - accuracy: 0.6593\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.0241 - accuracy: 0.5765\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6343\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.2395 - accuracy: 0.5348\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8503 - accuracy: 0.5724\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 3.6333 - accuracy: 0.5136\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.9650 - accuracy: 0.5749\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.2781 - accuracy: 0.5503\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7741 - accuracy: 0.5693\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.2253 - accuracy: 0.5017\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 4.0668 - accuracy: 0.5329\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.8851 - accuracy: 0.5092\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7265 - accuracy: 0.5217\n",
            "2420/2420 [==============================] - 10s 4ms/step - loss: 3.5936 - accuracy: 0.5499\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8812 - accuracy: 0.5324\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7396 - accuracy: 0.5115\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6993 - accuracy: 0.5290\n",
            "2420/2420 [==============================] - 10s 4ms/step - loss: 0.7902 - accuracy: 0.5918\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6978 - accuracy: 0.6392\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.6314 - accuracy: 0.5197\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7085 - accuracy: 0.5570\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.1705 - accuracy: 0.5269\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7632 - accuracy: 0.5248\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.8439 - accuracy: 0.5530\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.6568\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.3694 - accuracy: 0.5093\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 4.2474 - accuracy: 0.5193\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.8182 - accuracy: 0.5147\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7454 - accuracy: 0.5267\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0297 - accuracy: 0.5168\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8133 - accuracy: 0.5319\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7457 - accuracy: 0.6024\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6164 - accuracy: 0.6916\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 2.7338 - accuracy: 0.5174\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7980 - accuracy: 0.5867\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.3780 - accuracy: 0.5371\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7857 - accuracy: 0.6005\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.1471 - accuracy: 0.6265\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6936\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0074 - accuracy: 0.5738\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6678 - accuracy: 0.6753\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.4103 - accuracy: 0.5267\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7208 - accuracy: 0.5391\n",
            "2420/2420 [==============================] - 10s 4ms/step - loss: 1.9531 - accuracy: 0.5591\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7924 - accuracy: 0.5872\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 3.1597 - accuracy: 0.5062\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7434 - accuracy: 0.5314\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.7100 - accuracy: 0.6087\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7613 - accuracy: 0.6492\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9037 - accuracy: 0.6148\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.6528\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7439 - accuracy: 0.6100\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6877 - accuracy: 0.5741\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0586 - accuracy: 0.5972\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7034 - accuracy: 0.6574\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.7039 - accuracy: 0.5302\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7534 - accuracy: 0.5292\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.9827 - accuracy: 0.4976\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.9312 - accuracy: 0.4951\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.9265 - accuracy: 0.5083\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8361 - accuracy: 0.5230\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.2275 - accuracy: 0.4859\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8811 - accuracy: 0.4872\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.2688 - accuracy: 0.5113\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7040 - accuracy: 0.5021\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.2608 - accuracy: 0.5021\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7803 - accuracy: 0.5078\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7685 - accuracy: 0.5860\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6272\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.3767 - accuracy: 0.5828\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 3.9574 - accuracy: 0.5961\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.3687 - accuracy: 0.5176\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7260 - accuracy: 0.5177\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.5400 - accuracy: 0.5887\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6616 - accuracy: 0.6719\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.1561 - accuracy: 0.5352\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7920 - accuracy: 0.5894\n",
            "2420/2420 [==============================] - 9s 3ms/step - loss: 1.0701 - accuracy: 0.5380\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7586 - accuracy: 0.5328\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.9692 - accuracy: 0.4970\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7325 - accuracy: 0.5080\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.3119 - accuracy: 0.5622\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.6280\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.2959 - accuracy: 0.5443\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7828 - accuracy: 0.5625\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7648 - accuracy: 0.5117\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7126 - accuracy: 0.5133\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.1325 - accuracy: 0.5111\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8155 - accuracy: 0.4965\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.1439 - accuracy: 0.5416\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7481 - accuracy: 0.5645\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0109 - accuracy: 0.5125\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7686 - accuracy: 0.5113\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0635 - accuracy: 0.5371\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6951 - accuracy: 0.6371\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.5667 - accuracy: 0.5597\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6977 - accuracy: 0.6371\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.5816 - accuracy: 0.5513\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7639 - accuracy: 0.5712\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.1382 - accuracy: 0.4995\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7657 - accuracy: 0.5222\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.1882 - accuracy: 0.5086\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8177 - accuracy: 0.5093\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0860 - accuracy: 0.5900\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7198 - accuracy: 0.6201\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0468 - accuracy: 0.5087\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6969 - accuracy: 0.5114\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7820 - accuracy: 0.5143\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7283 - accuracy: 0.5120\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0722 - accuracy: 0.5917\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6897 - accuracy: 0.6357\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9590 - accuracy: 0.5360\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.6121\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.8630 - accuracy: 0.5373\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7050 - accuracy: 0.6112\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0600 - accuracy: 0.5672\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7412 - accuracy: 0.5722\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0837 - accuracy: 0.5895\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6750 - accuracy: 0.6592\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.2103 - accuracy: 0.5577\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6965 - accuracy: 0.5990\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.6614 - accuracy: 0.5163\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7576 - accuracy: 0.5484\n",
            "2420/2420 [==============================] - 10s 4ms/step - loss: 2.9901 - accuracy: 0.5766\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7109 - accuracy: 0.6446\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.2053 - accuracy: 0.5274\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8015 - accuracy: 0.5036\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.8343 - accuracy: 0.5831\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7077 - accuracy: 0.6153\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.0193 - accuracy: 0.5306\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7853 - accuracy: 0.5970\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9701 - accuracy: 0.5045\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7607 - accuracy: 0.5285\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7545 - accuracy: 0.5899\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6658\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.5513 - accuracy: 0.6241\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6102 - accuracy: 0.6830\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.1531 - accuracy: 0.5654\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7623 - accuracy: 0.6097\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7755 - accuracy: 0.6081\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6746 - accuracy: 0.6966\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7635 - accuracy: 0.6417\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.7055\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.2397 - accuracy: 0.5624\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.6633\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7551 - accuracy: 0.6207\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.6445\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.9252 - accuracy: 0.5432\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6748 - accuracy: 0.6049\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.1722 - accuracy: 0.5114\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7804 - accuracy: 0.5265\n",
            "2420/2420 [==============================] - 10s 4ms/step - loss: 0.8302 - accuracy: 0.5842\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.6368\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9487 - accuracy: 0.6208\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7782 - accuracy: 0.6298\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7560 - accuracy: 0.6785\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6149 - accuracy: 0.7213\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7625 - accuracy: 0.6228\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6774\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0083 - accuracy: 0.5895\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7007 - accuracy: 0.6836\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.0905 - accuracy: 0.5523\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 3.9020 - accuracy: 0.5822\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.2871 - accuracy: 0.5206\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 4.0651 - accuracy: 0.5436\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.1636 - accuracy: 0.5198\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6900 - accuracy: 0.6046\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.8005 - accuracy: 0.5311\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7144 - accuracy: 0.5380\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.8036 - accuracy: 0.5528\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6776 - accuracy: 0.6228\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9997 - accuracy: 0.5855\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7122 - accuracy: 0.6490\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 3.8734 - accuracy: 0.5033\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8127 - accuracy: 0.5238\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.2388 - accuracy: 0.5332\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 4.0489 - accuracy: 0.5269\n",
            "2420/2420 [==============================] - 10s 4ms/step - loss: 4.2044 - accuracy: 0.5215\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 1.1088 - accuracy: 0.5479\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.3643 - accuracy: 0.5196\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7002 - accuracy: 0.5269\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.3538 - accuracy: 0.5570\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 3.9518 - accuracy: 0.6132\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.8739 - accuracy: 0.5319\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7736 - accuracy: 0.5493\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 6.1049 - accuracy: 0.5111\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 3.4282 - accuracy: 0.5498\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 3.3498 - accuracy: 0.5196\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7588 - accuracy: 0.5222\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.1481 - accuracy: 0.5216\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 3.9875 - accuracy: 0.5307\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.1922 - accuracy: 0.5022\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.9387 - accuracy: 0.5043\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.1729 - accuracy: 0.5345\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7213 - accuracy: 0.5474\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.6253 - accuracy: 0.5001\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 4.1452 - accuracy: 0.5067\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 3.9272 - accuracy: 0.4975\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7030 - accuracy: 0.5140\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.7747 - accuracy: 0.5019\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.9830 - accuracy: 0.5239\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 6.3316 - accuracy: 0.5031\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 3.9913 - accuracy: 0.4973\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.8136 - accuracy: 0.5560\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7357 - accuracy: 0.5849\n",
            "2420/2420 [==============================] - 10s 4ms/step - loss: 1.1345 - accuracy: 0.5167\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8282 - accuracy: 0.5234\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 7.7377 - accuracy: 0.4984\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 7.7555 - accuracy: 0.4972\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.0805 - accuracy: 0.5247\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.6042\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.7453 - accuracy: 0.5341\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8023 - accuracy: 0.5392\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.3133 - accuracy: 0.5226\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8904 - accuracy: 0.5255\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.9346 - accuracy: 0.5094\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8219 - accuracy: 0.4983\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 3.1462 - accuracy: 0.5080\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8230 - accuracy: 0.5314\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.1444 - accuracy: 0.5122\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7554 - accuracy: 0.5141\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9429 - accuracy: 0.6268\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6193 - accuracy: 0.6786\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.2054 - accuracy: 0.5450\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7616 - accuracy: 0.6038\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.9797 - accuracy: 0.5245\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7736 - accuracy: 0.5751\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.7158 - accuracy: 0.5177\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7409 - accuracy: 0.5781\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 3.1861 - accuracy: 0.5132\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8957 - accuracy: 0.5437\n",
            "2420/2420 [==============================] - 10s 4ms/step - loss: 0.9219 - accuracy: 0.5078\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7649 - accuracy: 0.5166\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.8351 - accuracy: 0.6171\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6436 - accuracy: 0.6786\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.8613 - accuracy: 0.5641\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7651 - accuracy: 0.5933\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.0379 - accuracy: 0.5326\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7107 - accuracy: 0.6103\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 4.3710 - accuracy: 0.5302\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 3.9610 - accuracy: 0.5950\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7689 - accuracy: 0.5782\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.6389\n",
            "2420/2420 [==============================] - 10s 4ms/step - loss: 0.8507 - accuracy: 0.5931\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7757 - accuracy: 0.6551\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.2214 - accuracy: 0.5174\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7038 - accuracy: 0.5357\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7594 - accuracy: 0.6266\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6664\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7825 - accuracy: 0.6293\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6640 - accuracy: 0.6936\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.8068 - accuracy: 0.5325\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.8763 - accuracy: 0.5393\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.1040 - accuracy: 0.5997\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6810\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.8683 - accuracy: 0.5102\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7227 - accuracy: 0.5125\n",
            "2420/2420 [==============================] - 10s 4ms/step - loss: 4.3060 - accuracy: 0.4995\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 4.2471 - accuracy: 0.5031\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 3.4614 - accuracy: 0.5208\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7172 - accuracy: 0.5649\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7437 - accuracy: 0.5776\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6516\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.7783 - accuracy: 0.6452\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6840\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9131 - accuracy: 0.5517\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6681 - accuracy: 0.6354\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.5651 - accuracy: 0.5318\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.9036 - accuracy: 0.5316\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.5989 - accuracy: 0.5651\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6950 - accuracy: 0.6214\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.7983 - accuracy: 0.5114\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 4.3148 - accuracy: 0.4921\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 3.4742 - accuracy: 0.5845\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7260 - accuracy: 0.6803\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.8278 - accuracy: 0.5274\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7514 - accuracy: 0.5511\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.4135 - accuracy: 0.5349\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7304 - accuracy: 0.6177\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.9603 - accuracy: 0.5349\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7602 - accuracy: 0.5286\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.1310 - accuracy: 0.5262\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7477 - accuracy: 0.5536\n",
            "2420/2420 [==============================] - 10s 4ms/step - loss: 0.7833 - accuracy: 0.5403\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7068 - accuracy: 0.5767\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.8437 - accuracy: 0.6016\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7197 - accuracy: 0.6493\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 0.9688 - accuracy: 0.5247\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7501 - accuracy: 0.5393\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 2.3581 - accuracy: 0.5838\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.6813 - accuracy: 0.6675\n",
            "2420/2420 [==============================] - 9s 4ms/step - loss: 1.0587 - accuracy: 0.5436\n",
            "269/269 [==============================] - 1s 3ms/step - loss: 0.7509 - accuracy: 0.5145\n",
            "4302/4302 [==============================] - 16s 4ms/step - loss: 1.7652 - accuracy: 0.5495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params=fit.best_params_\n",
        "accuracy=fit.best_score_"
      ],
      "metadata": {
        "id": "2cCGEAPIBQWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh55bNtZPbPM",
        "outputId": "ad8876e6-8a54-4eda-83ba-079c7f8cd370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 20, 'nb_epoch': 200, 'unit': 15}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "id": "Y3thrwPzYrcV",
        "outputId": "0ed67a75-95b1-4f0f-d13d-cd398477e613",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6489305555820465"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Unit = 15\n",
        "\n",
        "# creating the layers of the NN\n",
        "ann = tf.keras.models.Sequential()\n",
        "ann.add(tf.keras.layers.Dense(units=15, activation='tanh'))\n",
        "ann.add(tf.keras.layers.Dense(units=15, activation='relu'))\n",
        "ann.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
        "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "rFX8jS7QRczQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=KerasClassifier(build_fn=ann, batch_size = 20, nb_epoch = 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmPj3FXFRHD6",
        "outputId": "61f11fde-9609-48ea-8a05-66ef065c8a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-547324f8013c>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model=KerasClassifier(build_fn=ann, batch_size = 20, nb_epoch = 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "SuZK08aMSLXN",
        "outputId": "bd75c74e-0633-4158-ddd6-9a46130f6ab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-d768f88d541e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid shape for y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m     elif (not isinstance(self.build_fn, types.FunctionType) and\n\u001b[1;32m    151\u001b[0m           not isinstance(self.build_fn, types.MethodType)):\n\u001b[0;32m--> 152\u001b[0;31m       self.model = self.build_fn(\n\u001b[0m\u001b[1;32m    153\u001b[0m           **self.filter_sk_params(self.build_fn.__call__))\n\u001b[1;32m    154\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_split_out_first_arg\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3098\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fn_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3099\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3100\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m   3101\u001b[0m           'The first argument to `Layer.call` must always be passed.')\n\u001b[1;32m   3102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The first argument to `Layer.call` must always be passed."
          ]
        }
      ]
    }
  ]
}